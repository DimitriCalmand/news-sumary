[
  {
    "title": "India leads the way on Google’s Nano Banana with a local creative twist",
    "url": "https://techcrunch.com/2025/09/17/india-leads-the-way-on-googles-nano-banana-with-a-local-creative-twist/",
    "content": "Google’s Nano Banana image-generation model, officially known as Gemini 2.5 Flash Image, has fueled global momentum for the Gemini app since launching last month. But in India, it has taken on a creative life of its own, with retro portraits and local trends going viral — even as privacy and safety concerns begin to emerge.\n--------------------\nIndia has emerged as the No. 1 country in terms of Nano Banana usage, according to David Sharon, multimodal generation lead for Gemini Apps at Google DeepMind, who spoke at a media session this week. The model’s popularity has also propelled the Gemini app to the top of the free app charts on both the App Store and Google Play in India. The app has also climbed to the top of global app stores’ charts, according to Appfigures.\n--------------------\nGiven India’s scale — the world’s second-largest smartphone market and second-biggest online population after China — it is no surprise the country is leading in adoption. But what is catching Google’s attention is not just how many people are using Nano Banana, it is how: Millions of Indians are engaging with the AI model in ways that are uniquely local, highly creative, and in some cases, completely unexpected.\n--------------------\nOne of the standout trends is Indians using Nano Banana to re-create retro looks inspired by 1990s Bollywood, imagining how they might have appeared during that era, complete with period-specific fashion, hairstyles, and makeup. This trend is local to India, Sharon told reporters.\n--------------------\nA variation of the retro trend is what some are calling the “AI saree,” where users generate vintage-style portraits of themselves wearing traditional Indian attire.\n--------------------\nAnother trend local to India is people generating their selfies in front of cityscapes and iconic landmarks, such as Big Ben and the U.K.’s retro telephone booths.\n--------------------\n“We saw a lot of that in the beginning,” Sharon said.\n--------------------\nIndian users are also experimenting with Nano Banana to transform objects, create time-travel effects, and even reimagine themselves as retro postage stamps. Others are generating black-and-white portraits or using the model to visualize encounters with their younger selves.\n--------------------\nSome of these trends did not originate in India, but the country played a key role in helping them gain global attention. One example is the figurine trend, where people generate miniature versions of themselves, often placing them in front of a computer screen. The trend first emerged in Thailand, spread to Indonesia, and became global after gaining traction in India, Sharon said.\n--------------------\nIn addition to Nano Banana, Google has observed a trend where Indian users are utilizing its Veo 3 AI video-generation model on the Gemini app to create short videos from old photos of their grandparents and great-grandparents.\n--------------------\nAll of this has helped drive Gemini’s popularity on both the App Store and Google Play in India. Between January and August, the app saw an average of 1.9 million monthly downloads in the country — about 55% higher than in the U.S. — accounting for 16.6% of global monthly downloads, per Appfigures data shared exclusively with TechCrunch.\n--------------------\nIndia downloads have totaled 15.2 million this year until August; the U.S., on the other hand, has had 9.8 million downloads so far this year, per Appfigures data.\n--------------------\nDaily downloads of the Gemini app in India significantly surged following the release of the Nano Banana update, beginning on September 1 with 55,000 installs across both app stores. Downloads peaked at 414,000 on September 13 — a 667% increase — with Gemini holding the top overall spot on the iOS App Store since September 10 and on Google Play since September 12, including across all categories, Appfigures data shows.\n--------------------\nDespite India leading in downloads, the country does not top in-app purchases on the Gemini app, which has generated an estimated $6.4 million in global consumer spending on iOS since launch, per Appfigures. The U.S. accounts for the largest share at $2.3 million (35%), while India contributes $95,000 (1.5%). However, India posted a record 18% month-over-month growth in spending, reaching $13,000 between September 1 and 16 — compared to an 11% global increase during the same period. That puts India seven percentage points above the global rate and more than 17 points ahead of the U.S., where growth was under 1%.\n--------------------\nThat said, as with other AI apps, there are concerns about users uploading personal photos to Gemini to transform their appearance.\n--------------------\n“When a user asks us to fulfill their query, we do our best to fulfill that query. We don’t try to assume what the user’s intent is,” Sharon said while addressing questions on how Google is dealing with data misuse and privacy concerns among users in India and other top markets. “We’ve really tried to improve that, and we have improved that to be bold and fulfil your request.”\n--------------------\nGoogle places a visible, diamond-shaped watermark on images generated by the Nano Banana model and also embeds a hidden marker using its SynthID tool to identify AI-generated content. SynthID allows Google to detect and flag whether an image was created using its models.\n--------------------\nSharon told reporters that Google is testing a detection platform with trusted testers, researchers, and other experts. The company also plans to launch a consumer-facing version that would allow anyone to check whether an image is AI-generated.\n--------------------\n“This is still day one, and we’re still learning, and we’re learning together. There are things that we might need to improve on in the future, and it’s really your feedback from users, press, academia, and experts that helps us improve,” Sharon said.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Meta unveils new smart glasses with a display and wristband controller",
    "url": "https://techcrunch.com/2025/09/17/meta-unveils-new-smart-glasses-with-a-display-and-wristband-controller/",
    "content": "Meta on Wednesday unveiled a new pair of Ray-Ban branded smart glasses with a built-in display for apps, alerts, and directions on the right lens. The smart glasses are controlled by a wristband that picks up on subtle hand gestures, called Meta Neural Band, the same one it unveiled at last year’s Connect as part of its Orion demo.\n--------------------\nCEO Mark Zuckerberg announced the new product, called Meta Ray-Ban Display, onstage at the company’s annual developer conference, Meta Connect 2025. Unlike Orion, Zuckerberg says this is a product that people can buy in a couple of weeks, starting September 30, and they’ll cost $799.\n--------------------\nThis is Meta’s latest attempt to ship a pair of consumer smart glasses that can handle many of the tasks users traditionally do on a smartphone. For years, Meta has been forced to reach users through its competitors’ devices, namely those sold by Google and Apple. While Meta has invested billions in virtual reality headsets, AI-powered smart glasses now seem like the most promising way for the company to connect with users on its own hardware.\n--------------------\nWith the Meta Ray-Ban Display, Meta aims to build off the success of its original Ray-Ban Meta smart glasses, which the company has sold millions of pairs of with its eyewear partner, EssilorLuxottica. Much like Ray-Ban Meta, the Meta Ray-Ban Display comes equipped with an on-board AI assistant, as well as cameras, speakers, and microphones. The glasses let users connect to the cloud to access the internet and social media apps.\n--------------------\nMeta says the display enables users to do much more with their smart glasses. Users are capable of displaying Meta apps like Instagram, WhatsApp, and Facebook. Users can also view directions and see live translations in the smart glasses’ display.\n--------------------\nThe Neural Band that ships alongside the device looks similar to a Fitbit, but without a screen, and allows users to navigate apps with small hand movements. Zuckerberg said onstage that the Meta Neural Band has 18 hours of battery life and is water resistant.\n--------------------\nThe device uses electromyography (EMG) to pick up on signals sent between your brain and your hand when performing a gesture. Meta is betting this interface will be a new way users can control their devices.\n--------------------\nEarlier this week, a video leaked of Meta’s latest smart glasses. CNBC and Bloomberg previously reported that the smart glasses, which were internally codenamed Hypernova, would be unveiled at this year’s Connect conference.\n--------------------\nIt’s worth noting that Meta Ray-Ban Display are far less capable than the Orion smart glasses Meta showed off at Connect 2024. That device featured augmented reality lenses and eye tracking, while this pair uses a much simpler display. It may be years before Meta ever sells Orion.\n--------------------\nStill, Meta is hoping it can win the smart glasses race by being first to market with a real product. However, it seems likely that Google and Apple will launch smart glasses of their own in the years to come. Those devices will undoubtedly be able to integrate into Google and Apple’s respective operating systems, giving them a significant leg up over Meta.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Nvidia AI chip challenger Groq raises even more than expected, hits $6.9B valuation",
    "url": "https://techcrunch.com/2025/09/17/nvidia-ai-chip-challenger-groq-raises-even-more-than-expected-hits-6-9b-valuation/",
    "content": "AI chip startup Groq confirmed Wednesday that it raised a fresh $750 million in funding at a post-money valuation of $6.9 billion.\n--------------------\nThis topped the rumored numbers when word leaked in July that Groq was raising. At that time, reports suggested that the raise would be about $600 million, at near a $6 billion valuation.\n--------------------\nGroq, which also sells data center computing power, previously raised $640 million at a $2.8 billion valuation in August 2024, making this more than double the valuation in about a year. Groq has now raised over $3 billion to date, PitchBook estimates.\n--------------------\nGroq has been a hot commodity because it is working on breaking the chokehold that AI chip maker Nvidia has over the tech industry. Groq’s chips are not GPUs, the graphics processing units that typically power AI systems. Instead, Groq calls them LPUs, (language processing units) and calls its hardware an inference engine — specialized computers optimized for running AI models quickly and efficiently.\n--------------------\nIts products are geared toward developers and enterprises, available as either a cloud service or an on-premises hardware cluster. The on-prem hardware is a server rack outfitted with a stack of its integrated hardware/software nodes. Both the cloud and on-prem hardware run open versions of popular models, like those from Meta, DeepSeek, Qwen, Mistral, Google and OpenAI. Groq says its offerings maintain, or in some cases improve, AI performance at significantly less cost than alternatives.\n--------------------\nGroq’s founder, Jonathan Ross, has a particularly relevant pedigree for this work. Ross previously worked at Google developing its Tensor Processing Unit chip, which are specialized processors designed for machine learning tasks. The TPU was announced in 2016, the same year Groq emerged from stealth. TPUs still power Google Cloud’s AI services.\n--------------------\nGroq says it now powers the AI apps of more than 2 million developers, up from 356,000 developers when the company talked to TechCrunch a year ago.\n--------------------\nThe new round was led by investment firm Disruptive, with additional funding from BlackRock, Neuberger Berman, Deutsche Telekom Capital Partners, and others. Existing investors, including Samsung, Cisco, D1, and Altimeter, also joined the round.\n--------------------\n\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Irregular raises $80 million to secure frontier AI models",
    "url": "https://techcrunch.com/2025/09/17/irregular-raises-80-million-to-secure-frontier-ai-models/",
    "content": "On Wednesday, AI security firm Irregular announced $80 million in new funding in a round led by Sequoia Capital and Redpoint Ventures, with participation from Wiz CEO Assaf Rappaport. A source close to the deal said the round valued Irregular at $450 million.\n--------------------\n“Our view is that soon, a lot of economic activity is going to come from human-on-AI interaction and AI-on-AI interaction,” co-founder Dan Lahav told TechCrunch, “and that’s going to break the security stack along multiple points.”\n--------------------\nFormerly known as Pattern Labs, Irregular is already a significant player in AI evaluations. The company’s work is cited in security evaluations for Claude 3.7 Sonnet as well as OpenAI’s o3 and o4-mini models. More generally, the company’s framework for scoring a model’s vulnerability-detection ability (dubbed SOLVE) is widely used within the industry.\n--------------------\nWhile Irregular has done significant work on models’ existing risks, the company is fundraising with an eye towards something even more ambitious: spotting emergent risks and behaviors before they surface in the wild. The company has constructed an elaborate system of simulated environments, enabling intensive testing of a model before it is released.\n--------------------\n“We have complex network simulations where we have AI both taking the role of attacker and defender,” says co-founder Omer Nevo. “So when a new model comes out, we can see where the defenses hold up and where they don’t.”\n--------------------\nSecurity has become a point of intense focus for the AI industry, as the potential risks posed by frontier models as more risks have emerged. OpenAI overhauled its internal security measures this summer, with an eye towards potential corporate espionage.\n--------------------\nAt the same time, AI models are increasingly adept at finding software vulnerabilities — a power with serious implications for both attackers and defenders.\n--------------------\nFor the Irregular founders, it’s the first of many security headaches caused by the growing capabilities of large language models.\n--------------------\n“If the goal of the frontier lab is to create increasingly more sophisticated and capable models, our goal is to secure these models,” Lahav says. “But it’s a moving target, so inherently there’s much, much, much more work to do in the future.”\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Kleiner Perkins-backed voice AI startup Keplar aims to replace traditional market research",
    "url": "https://techcrunch.com/2025/09/17/kleiner-perkins-backed-voice-ai-startup-keplar-aims-to-replace-traditional-market-research/",
    "content": "For decades, Fortune 500 companies had to hire market research firms to get meaningful insights into customer satisfaction. These services come with a hefty price tag and often take weeks to complete.\n--------------------\nKeplar, a market research startup, uses voice AI to conduct customer interviews, providing clients with analysis much faster and at a fraction of the cost of traditional research consulting firms. On Wednesday, the two-year-old company announced that it raised $3.4 million in seed funding led by Kleiner Perkins, with participation from SV Angel, Common Metal, and South Park Commons.\n--------------------\nThe idea for Keplar was conceived in 2023 when Dhruv Guliani (above right), previously an engineer at Google, where he worked on speech and voice AI models, and machine learning engineer William Wen, participated in the South Park Commons founder fellowship program.\n--------------------\nThe duo spoke with market researchers and brand managers and realized that the tools these professionals rely on — written surveys and interviews conducted by humans — can now be replaced by conversational AI.\n--------------------\nWith Keplar, companies can set up studies in minutes, Guliani told TechCrunch. The startup’s platform can turn any question about the product into an interview moderation guide. Keplar’s voice assistant will then reach out to participants and will ask them probing questions to understand what customers like and dislike about the product.\n--------------------\nIf Keplar is granted access to the client’s CRM, the AI voice researcher will reach out to existing customers. The results of AI conversations are then packaged into reports and PowerPoint presentations, similar to those traditionally provided by human market researchers.\n--------------------\nRelying on voice bots before advancements in LLMs would not have been possible. But voice AI has become so good that study participants sometimes forget they are speaking to AI, Guliani said.\n--------------------\n“These conversations feel really real. When you play everything back, you can even hear participants address the AI moderator by name: Ellie, Andrew or Ryan.”\n--------------------\nThe startup’s customers include Clorox and Intercom.\n--------------------\nKeplar isn’t the only AI company trying to disrupt the customer research market. Larger competitors include Outset, which raised a $17 million Series A led by 8VC in June, and Listen Labs, which raised $27 million from Sequoia in April.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Meet Macroscope: an AI tool for understanding your code base, fixing bugs",
    "url": "https://techcrunch.com/2025/09/17/meet-macroscope-an-ai-tool-for-understanding-your-code-base-fixing-bugs/",
    "content": "The founders who previously sold their livestreaming video startup Periscope to Twitter are back with a new startup — and no surprise, it’s an AI-focused company this time around.\n--------------------\nOn Wednesday, former Twitter head of product Kayvon Beykpour announced the launch of Macroscope, an AI system aimed at developers and product leaders that summarizes updates to a codebase and catches bugs, among other things.\n--------------------\nThe startup was co-founded by Beykpour, now Macroscope CEO, in July 2023, along with childhood friend Joe Bernstein, also previously of Periscope and their prior enterprise startup, Terriblyclever, which was sold to Blackboard in 2009. They’re joined by co-founder Rob Bishop, who sold his computer vision and machine learning company, Magic Pony Technology, to Twitter in 2016.\n--------------------\nThe company describes its product as an “AI-powered understanding engine” that’s designed to save engineers time, and the type of product the founders “wish we’d had” when building their earlier companies.\n--------------------\nToday, engineers use a variety of tools to keep track of work, like JIRA, Linear, and spreadsheets, and spend too much time in meetings instead of building, Beykpour says. Macroscope is designed to fix this.\n--------------------\n“I feel like I lived this pain…at every company I worked at, whether it was the startups that we built ourselves, or whether it was enormous public companies like Twitter, we sort of lived this problem the hard way,” Beykpour told TechCrunch in an interview.\n--------------------\n“Trying to get a sense for what everyone was doing, especially when you have an organization like Twitter with thousands of engineers, it was literally most of my job — and my least favorite part of my job as the head of product at Twitter,” he said.\n--------------------\nTo address this issue and others, Macroscope’s customers first install its GitHub app, which gives the company access to the code base. They can then optionally install other integrations, like a Slack app, Linear app, and JIRA app. The software then does the rest of the work by analyzing the code and noting what’s changing.\n--------------------\nThis involves a process called code walking, which uses the Abstract Syntax Tree (AST) — a structural representation of programming code — to gather important context about how the customer’s code base works. That knowledge is then used in conjunction with large language models (LLMs).\n--------------------\nOnce up and running, engineers can use Macroscope to discover bugs to fix in their PRs (pull requests), summarize their PRs, get a summary of how the codebase is changing, and ask code research-based questions. Meanwhile, product leaders could use the software to get real-time summaries of product updates, productivity insights, answers to natural language questions about the product, code, or development activity, and more. This can help them determine what teams are prioritizing in terms of engineering allocation.\n--------------------\n“You can ask natural language questions, regardless of what your technical ability is,” notes Beykpour. “This might be very useful if you’re trying to learn about the code base without distracting a senior engineer on your team. Very valuable. If you’re a CEO and you want to understand literally, ‘what did we get done this week?’, your options are either ask Macroscope or go distract some teammates,” he adds. “One is a lot more expensive than the other.”\n--------------------\nWhile there isn’t a product that offers a direct competitor to all that Macroscope offers, it does compete in the code review space — where developers examine and test code changes before they’re implemented — with tools like CodeRabbit, Cursor Bugbot, Graphite Diamond, Greptile, and others. However, the company said when it ran its own internal benchmark of over 100 real-world bugs, its product caught 5% more bugs than the next-best tool. It also generated 75% fewer comments. (It shared its benchmark publicly in a blog post.)\n--------------------\nThe software costs $30 per active developer per month, starting at five seats, and offers enterprise pricing and custom integrations for larger businesses. It requires the use of GitHub Cloud. Ahead of its launch, a number of startups and larger firms have been using the product, including XMTP, Things, United Masters, Bilt, Class.com, Seed.com, ParkHub, A24 Labs, and others.\n--------------------\nThe San Francisco-based startup has a team of 20 and is backed by $30 million in Series A funding, which was closed in July and led by Michael Mignano at Lightspeed. Other investors include Adverb, Thrive Capital, and Google Ventures. To date, Macroscope has raised $40 million total.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "China tells its tech companies they can’t buy AI chips from Nivida",
    "url": "https://techcrunch.com/2025/09/17/china-tells-its-tech-companies-they-cant-buy-ai-chips-from-nivida/",
    "content": "Nvidia just got shut out of the Chinese market — this time by the Chinese government instead of the US.\n--------------------\nChina’s internet regulator, the Cyberspace Administration of China, banned domestic tech companies from buying Nvidia AI chips on Wednesday, as first reported by the Financial Times.\n--------------------\nThe agency also told tech companies including ByteDance and Alibaba to stop testing and ordering Nvidia’s RTX Pro 6000D server, a device designed specifically for the market in China.\n--------------------\nBeijing had previously discouraged companies from buying these chips in late August, instead promoting alternatives from local manufacturers.\n--------------------\nThis ban will deliver quite a blow to China’s tech ecosystem. While companies like Huawei and Alibaba design AI chips locally, Nvidia is by far the global market leader, and its chips are considered to be some of the most advanced on the market.\n--------------------\nWhen asked for comment, Nvidia provided the following statement from CEO Jensen Huang at a press conference on Wednesday: “We can only be in service of a market if a country wants us to be,” Huang said. “I’m disappointed with what I see but they have larger agendas to work out between China and the United States. And I’m patient about it. We’ll continue to be supportive of the Chinese government and Chinese companies as they wish.”\n--------------------\nThe Trump administration hit semiconductor companies, including Nvidia, with licensing requirements to sell their AI chips in China in April.\n--------------------\nOn Nvidia’s first-quarter earnings call, Huang had said Nvidia was going to endure $8 billion of revenue loss in the second quarter alone by not being able to sell its H20 AI chips in China.\n--------------------\nIn June, Nvidia said that it wouldn’t include China in its future profit and forecast as it was essentially locked out of the market.\n--------------------\nIn July, the Trump administration reversed course and gave semiconductor companies the green light to sell their chips in China again. In August, the White House announced it would grant the licenses needed to sell in China, but with a catch: the U.S. government would get 15% of the revenue from the chips sold. But as of Nvidia’s latest earnings, the company had yet to sell any units to Chinese customers under the plan, citing the slow implementation of President Trump’s proposal.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "AI and the Future of Defense: Mach Industries’ Ethan Thornton at TechCrunch Disrupt 2025",
    "url": "https://techcrunch.com/2025/09/17/the-new-face-of-defense-tech-takes-the-ai-stage-at-techcrunch-disrupt-2025/",
    "content": "From stealth mode to center stage, Mach Industries is bringing AI into one of the world’s most complex and controversial sectors: defense. At TechCrunch Disrupt 2025, Ethan Thornton, CEO and founder of Mach Industries, steps onto the AI Stage to share what it takes to build in high-stakes environments where speed and autonomy matter most — and why next-gen infrastructure starts with rethinking the fundamentals.\n--------------------\nThornton launched Mach Industries out of MIT in 2023 with a bold mission: to build decentralized, next-generation defense technologies that safeguard freedom on a global scale. Now leading one of the most ambitious startups in the space, he’s bringing startup speed and AI-native innovation to an industry long dominated by legacy players.\n--------------------\nMach Industries  is part of a new wave proving that AI startups can play a critical role in national defense. This session will unpack what that means — from autonomous systems and edge computing to dual-use technologies that blur the lines between commercial and military capability. Thornton will dig into funding, regulation, and responsibility at the intersection of tech and geopolitics.\n--------------------\nWith global tensions rising and defense investment accelerating, this conversation offers a timely look at how AI is reshaping security, strategy, and sovereignty. Don’t miss it live on the AI Stage, October 27–29 at Moscone West in San Francisco. Register now to join 10,000+ startup and VC leaders — and save up to $668 before prices increase after September 26.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Amazon launches AI agent to help sellers complete tasks and manage their businesses",
    "url": "https://techcrunch.com/2025/09/17/amazon-launches-ai-agent-to-help-sellers-complete-tasks-and-manage-their-businesses/",
    "content": "Amazon announced on Wednesday that it’s introducing an always-on AI agent that will help sellers on its platform run their businesses. The company is updating Seller Assistant, its AI tool for third-party sellers, to help handle tasks on the seller’s behalf.\n--------------------\n“Our agentic AI capabilities are designed to work seamlessly throughout the entire selling experience, which means sellers can go from handling every task themselves to collaborating with an intelligent assistant that works proactively on their behalf around the clock, while always keeping sellers in control,” Amazon wrote in a press release. “Seller Assistant will be able to handle everything from routine operations to complex business strategy, so sellers can focus on innovation and growth.”\n--------------------\nSeller Assistant can now not only monitor account health and inventory, but also help develop strategies and take action when authorized, Amazon says.\n--------------------\nFor example, when a seller is reviewing their inventory, Seller Assistant will flag slow-moving products before they incur long-term storage fees and recommend whether it would make sense to leave the item as it is, lower the price, or remove it altogether. Seller Assistant will also be able to analyze demand patterns and prepare shipment recommendations.\n--------------------\nSeller Assistant continuously monitors a seller’s account and flag potential issues and actions, such as inventory listings that might violate new product safety regulations. Additionally, it can automatically ensure that all of a seller’s products meet compliance requirements in every country they’re selling in.\n--------------------\nAgent-driven commerce is an area of intense interest for tech companies, which imagine a future in which agents can initiate deals or make purchases on behalf of their clients. On Tuesday, Google released a new payments protocol for agentic transactions, although Amazon was not named as a partner.\n--------------------\nAmazon also announced that it’s bringing agentic AI to advertising, allowing sellers to develop ads through conversational prompts.\n--------------------\nToday’s announcement marks the latest AI tools that Amazon has rolled out for third-party sellers on its platform. Other tools include a video generator for ads and a generative AI tool that helps merchants improve their product listings.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Lovable co-founder and CEO Anton Osika on building one of the fastest-growing startups in history at TechCrunch Disrupt 2025",
    "url": "https://techcrunch.com/2025/09/17/lovable-ceo-anton-osika-on-building-one-of-the-fastest-growing-startups-in-history-at-techcrunch-disrupt-2025/",
    "content": "Lovable has quickly become one of the most talked-about startups of the year, breaking records and making headlines as one of the fastest-growing software companies in history. To mark the 20th anniversary of TechCrunch, co-founder and CEO Anton Osika will take the Disrupt Stage to discuss Lovable and the Future of Consumer Tech. Join us at TechCrunch Disrupt 2025, October 27–29 at Moscone West in San Francisco, alongside 10,000+ founders, investors, and tech leaders shaping what’s next.\n--------------------\nLovable lets anyone create apps and websites simply by talking to AI. Its mission: empower the 99% of people who can’t code to turn their ideas into software. The momentum has been staggering. Lovable reached $100 million ARR in under a year, raised a $200 million Series A at a $1.8 billion valuation led by Accel, and has been fielding unsolicited investor offers that push its valuation toward $4 billion. As TechCrunch recently reported, investors are “loving Lovable” — and it’s easy to see why.\n--------------------\nA physicist-turned-entrepreneur, Osika co-founded Lovable after earlier roles as co-founder of Depict.ai, Founding Engineer at Sana, and a particle physicist at CERN. Based in Stockholm, he’s built Lovable into a global story at lightning speed, blending technical depth with a consumer-first vision. In this session, he’ll share what it takes to build a brand that not only scales in a competitive market but also navigates the cultural conversations that come with such rapid success.\n--------------------\nLovable’s rise isn’t just a startup story — it’s a blueprint for the next wave of consumer tech. Osika’s journey offers rare insight into how to scale at breakneck speed, balance investor pressure with product focus, and carve out a category others have long ignored. For anyone building consumer experiences, this will be a masterclass.\n--------------------\nCatch Anton Osika on the Disrupt Stage this October and see how Lovable is redefining consumer tech. Secure your pass today and save up to $668 before Regular Bird rates end September 26.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Sonair built its 3D ultrasonic sensor with robotic safety in mind",
    "url": "https://techcrunch.com/2025/09/17/sonair-built-its-3d-ultrasonic-sensor-with-robotic-safety-in-mind/",
    "content": "As robots increasingly enter human spaces, robotics companies will need to think about safety differently than they did when robots were largely siloed from their human counterparts.\n--------------------\nSonair thinks its sensors can help robotics companies reach their safety goals — with a solution that is both better and cheaper than popular LIDAR technology.\n--------------------\nThe Oslo, Norway-based company built an ADAR (acoustic detection and ranging) sensor for robots that uses high frequency sound. These sensors send out ultrasound waves and capture how the sound echoes back. These signals give robots a three-dimensional view of their surroundings.\n--------------------\nThis data complements a robot’s other sensors and cameras to give the robot’s operating system a clearer picture of the environment.\n--------------------\n“Perception for a human — what we’re using the most is our eyes, but we’re also using other senses to perceive our environment, our ears and our brain to interpret all our senses,” said Sonair co-founder and CEO Knut Sandven in an interview with TechCrunch. “The same is for robots or autonomous machines. They’re using cameras. Cameras are really great to understand the environment, but they’re not good for reliably detecting objects under all circumstances.”\n--------------------\nSonair is designed to help fill those gaps — especially for depth perception. Traditionally, robotics companies turn to LIDAR sensors, which send out beams of light and measure how they bounce back, to gather that information. Sandven said Sonair’s sensors are a better option because they can capture more comprehensive data.\n--------------------\n“LIDAR is like swiping a laser pointer,” Sandven said. “[But] if you shout out in a room, you will fill the room with sound. We will fill the room with sound.”\n--------------------\nThe sensor’s output is structured in a standard industry format, Sandven said, so it’s designed to work alongside a variety of different robotic hardware and software.\n--------------------\nThe company released its sensor earlier this year and has since seen strong demand from the robotics field, with multiple companies planning to incorporate Sonair’s sensors into their next robot models, Sandven said.\n--------------------\nSonair has also seen demand from the industrial safety sector. Sandven said companies are using the sensors to detect when people enter areas with heavy machinery so the machines can be shut off automatically before an accident happens.\n--------------------\nNow, Sonair is looking to scale up adoption of its tech and just raised a $6 million round to do so. The round included new and returning investors Scale Capital, Norway’s state-backed Investinor, and ProVenture, among others.\n--------------------\nSandven said that investors who are active in the robotics space immediately understood the problem that the company is looking to solve. This isn’t surprising as safety will likely become a major concern as robots start interacting with humans more — not unlike the safety conversations that emerged in the early days of the self-driving car industry.\n--------------------\nFady Saad, a general partner, at robotics-focused Cybernetix Ventures, which is not an investor in Sonair, recently told TechCrunch that potential safety concerns were one of the reasons he doesn’t expect people to want humanoid robots in their home anytime soon.\n--------------------\nThe “kind of dirty secret of humanoids at homes s there’s a lot of safety, lots of security, lots of concerns,” Saad told TechCrunch in August. “If this thing falls on pets or kids, it will hurt them, right? This is just one aspect of a big hurdle that no one is paying attention to, or very few people are paying attention to.”\n--------------------\nSandven said Sonair doesn’t currently have direct competition for its sonar-based sensors, but that could change as more companies try to find safety solutions for robots.\n--------------------\n“My goal is to have this technology in all robots, like you have with cameras,” Sandven said. “If we talk again this time next year, we will have a pretty good indication whether that is the direction we are heading.”\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Google Ventures doubles down on dev tool startup Blacksmith just 4 months after its seed round",
    "url": "https://techcrunch.com/2025/09/17/google-ventures-doubles-down-on-dev-tool-startup-blacksmith-just-4-months-after-its-seed-round/",
    "content": "As speed becomes the defining currency in an AI-driven software world, Blacksmith has raised another round led by Google Ventures — just four months after its seed — to accelerate how code gets shipped.\n--------------------\nThe $10 million Series A closed in just 14 days, with Google Ventures doubling down after first backing Blacksmith’s $3.5 million seed in May. At the time, Alphabet’s VC arm bet on the size of the market and the founding team, which included veterans of Cockroach Labs, another GV portfolio company. But for this round, GV was swayed by results.\n--------------------\nBlacksmith, which offers a continuous integration and continuous delivery service for developers that complements GitHub actions, had pulled in hundreds of customers since May, and the boom in AI coding agents has blown the market wide open, co-founder and CEO Aditya Jayaprakash (pictured above on the left) said in an exclusive interview.\n--------------------\nThe San Francisco–based startup hit $1 million in annual recurring revenue (ARR) in February with just four people — Jayaprakash, co-founders Aayush Shah and Aditya Maru, and a product designer. Since then, revenue has reached $3.5 million ARR with more than 700 customers, supported by a team of eight, and the company is aiming to double that figure by year’s end, Jayaprakash told TechCrunch.\n--------------------\nFounded in January 2024, Blacksmith was born from the experiences of its founders, who met at the University of Waterloo before building large-scale distributed systems at Faire and Cockroach Labs. There, they saw firsthand how costly and unpredictable the build and unit testing stages of software releases, known as continuous integration (CI), can be.\n--------------------\nYou would have to spin up hundreds of machines and burn through hundreds of hours of computing power just to test new code before shipping it, Jayaprakash said.\n--------------------\nA typical software development process involves developers continuously pushing new code into repositories such as GitHub or AWS CodeCommit. To manage the testing and integration of that code, cloud service providers such as Amazon Web Services, Google Cloud Platform, and Microsoft Azure all offer their own solutions — but these are often slower, costlier, or less predictable than teams need.\n--------------------\nUnlike many rivals that rent generic cloud servers from cloud providers like AWS, Blacksmith’s service runs on  high-performance, gaming-grade CPUs. The result, the startup says, is up to double the processing speed and lowering, by as much as 75%, compute costs. And because teams can switch by changing just a single line of code, they can start shipping faster within minutes.\n--------------------\n“Because we’re going the bare-metal route, we have much better control over our economics compared to the hyperscalers,” Jayaprakash told TechCrunch. “I’m not saying every company should go bare metal… but if you are a compute company, if you are an infra company, where your bread and butter is compute, like ourselves, it makes a lot of sense, and it gives us abundant control over our margins.”\n--------------------\nBy using hardware at its premises, the startup improves its margins as it grows its customer base, the founder said.\n--------------------\nBlacksmith also offers test analytics and an observability roadmap, giving customers deeper insights into GitHub Actions — GitHub’s CI/CD platform that automates how developers test and deploy software.\n--------------------\nBlacksmith targets companies with teams of 500 engineers or more. Customers already running their GitHub Actions through the platform include Ashby, Chroma, Clerk, Devsisters, Mintlify, Pylon, Slope, Supabase, and VEED.\n--------------------\nThe latest funding round also saw participation from existing investors and angels, including Spencer Kimball, CEO of Cockroach Labs, and David Cramer, co-founder of Sentry. Blacksmith launched out of Y Combinator’s Winter 2024 batch and today has a team of 11.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Meta Connect 2025: What to expect and how to watch",
    "url": "https://techcrunch.com/2025/09/16/meta-connect-2025-what-to-expect-and-how-to-watch/",
    "content": "Meta Connect 2025 — the company’s biggest conference of the year where it unveils smart glasses and VR headsets — kicks off on Wednesday night. We’re expecting the star of Meta Connect 2025 to be the company’s new AI-powered smart glasses with Ray-Ban and Oakley, but the company may have some other surprises in store regarding the Metaverse, Quest headsets, or even its broader AI ambitions.\n--------------------\nMeta says it has sold millions of Ray-Ban Meta smart glasses, and earlier this year, Meta unveiled its latest AI-powered smart glasses with Oakley, which were designed for athletes. Silicon Valley is leaning heavily into AI wearables, and Meta seems to be one of the companies leading the charge.\n--------------------\nNotably, this is the company’s first Connect conference since it started Meta Superintelligence Labs (MSL), its boldest effort yet to develop cutting-edge AI systems under former Scale AI CEO Alexandr Wang. It’s possible we’ll get some official updates on how that project is going, and we may hear from some MSL executives.\n--------------------\nAs Meta looks to regain its footing in the AI race, this year’s Connect feels especially consequential.\n--------------------\nMeta Connect 2025 starts at 5 p.m. PT Wednesday, with a keynote from CEO Mark Zuckerberg. The event will take place in person at Meta’s headquarters in Menlo Park. You can register for free to watch the livestream virtually on Meta’s website. Meta’s agenda says the keynote will be roughly an hour.\n--------------------\nIf you want to get that Menlo Park feel from the comfort of your living room, you can also access the keynote through Horizon via your Meta Quest headset. You can also access the Meta Connect 2025 keynote on Facebook via the company’s official developer page, Meta for Developers.\n--------------------\nOn Thursday, Meta will host a Developer Keynote staring 10 a.m. PT to discuss the new experiences people can build with its devices. Then, at 10:45 a.m. PT, Meta will host a conversation between Chief Scientist of Reality Labs Michael Abrash and VP of Reality Labs Research Richard Newcombe. The two Meta executives are slated to discuss the “future of glasses with contextual AI, and how Meta is poised to transform the future of computing.”\n--------------------\nThere have already been several leaks regarding what will be announced at Meta Connect 2025. Perhaps the biggest pertains to a new type of smart glasses called Hypernova.\n--------------------\nA now-removed video on Meta’s YouTube channel, spotted by UploadVR, showed a pair of Ray-Ban Meta smart glasses with a heads-up display on the right lens, as well as cameras, microphones, and an onboard AI assistant.\n--------------------\nThe glasses in the video were controlled by a wristband, which was unveiled at last year’s Connect and is controlled by subtle hand gestures.\n--------------------\nThe video suggests that Meta will unveil, and perhaps launch, the Hypernova glasses this week. CNBC previously reported that Meta was planning to unveil Hypernova and launch the wristband at Connect 2025.\n--------------------\nMeta also seems likely to unveil new pair of smart glasses it developed with Oakley at Connect 2025. The companies are expected to launch a new pair of AI-powered smart glasses in Oakley’s Spheara style. This features a large unified lens on the front — an ideal shape for runners and bikers. Unlike previous Meta smart glasses, this model has one centered camera above the nose bridge, rather than two cameras on the top corners of the frames.\n--------------------\nOn the VR front, it’s unclear whether Meta will release any new Quest headsets as part of this year’s Connect. Even though the conference, and company, is named after the Metaverse, it seems like that’s less of the focus this year. Meta is reportedly developing an ultralight VR headset for launch by the end of 2026. However, the company could wait to show that off at next year’s Connect.\n--------------------\nThat said, Meta promises that Zuckerberg will talk about the Metaverse in some shape or form. We don’t doubt that.\n--------------------\nAs for Meta’s AI ambitions, it wouldn’t be surprising if Zuckerberg used Connect 2025 as a chance to highlight all the work MSL is doing. The company’s first LlamaCon, its AI developer conference, took place earlier this year before Meta invested billions in Scale AI and hired researchers from around the industry.\n--------------------\nRight now, Meta’s standalone AI app is in a confusing spot where it both controls smart glasses and can be used as an AI chatbot. It’s possible that app also gets some updates that make it easier to use.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Silicon Valley bets big on ‘environments’ to train AI agents",
    "url": "https://techcrunch.com/2025/09/16/silicon-valley-bets-big-on-environments-to-train-ai-agents/",
    "content": "For years, Big Tech CEOs have touted visions of AI agents that can autonomously use software applications to complete tasks for people. But take today’s consumer AI agents out for a spin, whether it’s OpenAI’s ChatGPT Agent or Perplexity’s Comet, and you’ll quickly realize how limited the technology still is. Making AI agents more robust may take a new set of techniques that the industry is still discovering.\n--------------------\nOne of those techniques is carefully simulating workspaces where agents can be trained on multi-step tasks — known as reinforcement learning (RL) environments. Similarly to how labeled datasets powered the last wave of AI, RL environments are starting to look like a critical element in the development of agents.\n--------------------\nAI researchers, founders, and investors tell TechCrunch that leading AI labs are now demanding more RL environments, and there’s no shortage of startups hoping to supply them.\n--------------------\n“All the big AI labs are building RL environments in-house,” said Jennifer Li, general partner at Andreessen Horowitz, in an interview with TechCrunch. “But as you can imagine, creating these datasets is very complex, so AI labs are also looking at third party vendors that can create high quality environments and evaluations. Everyone is looking at this space.”\n--------------------\nThe push for RL environments has minted a new class of well-funded startups, such as Mechanize and Prime Intellect, that aim to lead the space. Meanwhile, large data-labeling companies like Mercor and Surge say they’re investing more in RL environments to keep pace with the industry’s shifts from static datasets to interactive simulations. The major labs are considering investing heavily too: according to The Information, leaders at Anthropic have discussed spending more than $1 billion on RL environments over the next year.\n--------------------\nThe hope for investors and founders is that one of these startups emerge as the “Scale AI for environments,” referring to the $29 billion data labelling powerhouse that powered the chatbot era.\n--------------------\nThe question is whether RL environments will truly push the frontier of AI progress.\n--------------------\nAt their core, RL environments are training grounds that simulate what an AI agent would be doing in a real software application. One founder described building them in recent interview “like creating a very boring video game.”\n--------------------\nFor example, an environment could simulate a Chrome browser and task an AI agent with purchasing a pair of socks on Amazon. The agent is graded on its performance and sent a reward signal when it succeeds (in this case, buying a worthy pair of socks).\n--------------------\nWhile such a task sounds relatively simple, there are a lot of places where an AI agent could get tripped up. It might get lost navigating the web page’s drop down menus, or buy too many socks. And because developers can’t predict exactly what wrong turn an agent will take, the environment itself has to be robust enough to capture any unexpected behavior, and still deliver useful feedback. That makes building environments far more complex than a static dataset.\n--------------------\nSome environments are quite elaborate, allowing for AI agents to use tools, access the internet, or use various software applications to complete a given task. Others are more narrow, aimed at helping an agent learn specific tasks in enterprise software applications.\n--------------------\nWhile RL environments are the hot thing in Silicon Valley right now, there’s a lot of precedent for using this technique. One of OpenAI’s first projects back in 2016 was building “RL Gyms,” which were quite similar to the modern conception of environments. The same year, Google DeepMind’s AlphaGo AI system beat a world champion at the board game, Go. It also used RL techniques within a simulated environment.\n--------------------\nWhat’s unique about today’s environments is that researchers are trying to build computer-using AI agents with large transformer models. Unlike AlphaGo, which was a specialized AI system working in a closed environments, today’s AI agents are trained to have more general capabilities. AI researchers today have a stronger starting point, but also a complicated goal where more can go wrong.\n--------------------\nAI data labeling companies like Scale AI, Surge, and Mercor are trying to meet the moment and build out RL environments. These companies have more resources than many startups in the space, as well as deep relationships with AI labs.\n--------------------\nSurge CEO Edwin Chen tells TechCrunch he’s recently seen a “significant increase” in demand for RL environments within AI labs. Surge — which reportedly generated $1.2 billion in revenue last year from working with AI labs like OpenAI, Google, Anthropic and Meta — recently spun up a new internal organization specifically tasked with building out RL environments, he said.\n--------------------\nClose behind Surge is Mercor, a startup valued at $10 billion, which has also worked with OpenAI, Meta, and Anthropic. Mercor is pitching investors on its business building RL environments for domain specific tasks such as coding, healthcare, and law, according to marketing materials seen by TechCrunch.\n--------------------\nMercor CEO Brendan Foody told TechCrunch in an interview that “few understand how large the opportunity around RL environments truly is.”\n--------------------\nScale AI used to dominate the data labeling space, but has lost ground since Meta invested $14 billion and hired away its CEO. Since then, Google and OpenAI dropped Scale AI as a data provider, and the startup even faces competition for data labelling work inside of Meta. But still, Scale is trying to meet the moment and build environments.\n--------------------\n“This is just the nature of the business [Scale AI] is in,” said Chetan Rane, Scale AI’s head of product for agents and RL environments. “Scale has proven its ability to adapt quickly. We did this in the early days of autonomous vehicles, our first business unit. When ChatGPT came out, Scale AI adapted to that. And now, once again, we’re adapting to new frontier spaces like agents and environments.”\n--------------------\nSome newer players are focusing exclusively on environments from the outset. Among them is Mechanize, a startup founded roughly six months ago with the audacious goal of “automating all jobs.” However, co-founder Matthew Barnett tells TechCrunch that his firm is starting with RL environments for AI coding agents.\n--------------------\nMechanize aims to supply AI labs with a small number of robust RL environments, Barnett says, rather than larger data firms that create a wide range of simple RL environments. To this point, the startup is offering software engineers $500,000 salaries to build RL environments — far higher than an hourly contractor could earn working at Scale AI or Surge.\n--------------------\nMechanize has already been working with Anthropic on RL environments, two sources familiar with the matter told TechCrunch. Mechanize and Anthropic declined to comment on the partnership.\n--------------------\nOther startups are betting that RL environments will be influential outside of AI labs. Prime Intellect — a startup backed by AI researcher Andrej Karpathy, Founders Fund, and Menlo Ventures — is targeting smaller developers with its RL environments.\n--------------------\nLast month, Prime Intellect launched an RL environments hub, which aims to be a “Hugging Face for RL environments.” The idea is to give open-source developers access to the same resources that large AI labs have, and sell those developers access to computational resources in the process.\n--------------------\nTraining generally capable agents in RL environments can be more computational expensive than previous AI training techniques, according to Prime Intellect researcher Will Brown. Alongside startups building RL environments, there’s another opportunity for GPU providers that can power the process.\n--------------------\n“RL environments are going to be too large for any one company to dominate,” said Brown in an interview. “Part of what we’re doing is just trying to build good open-source infrastructure around it. The service we sell is compute, so it is a convenient onramp to using GPUs, but we’re thinking of this more in the long term.”\n--------------------\nThe open question around RL environments is whether the technique will scale like previous AI training methods.\n--------------------\nReinforcement learning has powered some of the biggest leaps in AI over the past year, including models like OpenAI’s o1 and Anthropic’s Claude Opus 4. Those are particularly important breakthroughs because the methods previously used to improve AI models are now showing diminishing returns.\n--------------------\nEnvironments are part of AI labs’ bigger bet on RL, which many believe will continue to drive progress as they add more data and computational resources to the process. Some of the OpenAI researchers behind o1 previously told TechCrunch that the company originally invested in AI reasoning models — which were created through investments in RL and test-time-compute — because they thought it would scale nicely.\n--------------------\nThe best way to scale RL remains unclear, but environments seem like a promising contender. Instead of simply rewarding chatbots for text responses, they let agents operate in simulations with tools and computers at their disposal. That’s far more resource-intensive, but potentially more rewarding.\n--------------------\nSome are skeptical that all these RL environments will pan out. Ross Taylor, a former AI research lead with Meta that co-founded General Reasoning, tells TechCrunch that RL environments are prone to reward hacking. This is a process in which AI models cheat in order to get a reward, without really doing the task.\n--------------------\n“I think people are underestimating how difficult it is to scale environments,” said Taylor. “Even the best publicly available [RL environments] typically don’t work without serious modification.”\n--------------------\nOpenAI’s Head of Engineering for its API business, Sherwin Wu, said in a recent podcast that he was “short” on RL environment startups. Wu noted that it’s a very competitive space, but also that AI research is evolving so quickly that it’s hard to serve AI labs well.\n--------------------\nKarpathy, an investor in Prime Intellect that has called RL environments a potential breakthrough, has also voiced caution for the RL space more broadly. In a post on X, he raised concerns about how much more AI progress can be squeezed out of RL.\n--------------------\n“I am bullish on environments and agentic interactions but I am bearish on reinforcement learning specifically,” said Karpathy.\n--------------------\nUpdate: A previous version of this article referred to Mechanize as Mechanize Work. It has been updated to reflect the company’s official name.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "CodeRabbit raises $60M, valuing the 2-year-old AI code review startup at $550M",
    "url": "https://techcrunch.com/2025/09/16/coderabbit-raises-60m-valuing-the-2-year-old-ai-code-review-startup-at-550m/",
    "content": "Harjot Gill was running FluxNinja, an observability startup he co-founded several years after selling his first startup Netsil to Nutanix in 2018, when he noticed a curious trend.\n--------------------\n“We had a team of remote engineers who were starting to adopt AI code generation on GitHub Copilot,” Gill told TechCrunch. “We saw that adoption happen, and it was very clear to me that as a second-order effect, it’s going to cause bottlenecks in the code review.”\n--------------------\nIn early 2023, Gill started CodeRabbit, an AI-powered code review platform, and it acquired FlexNinja.\n--------------------\nGill’s prediction has come true: developers are now regularly using AI coding assistants to generate code, but the output is often buggy, forcing engineers to spend a lot of time on corrections.\n--------------------\nCodeRabbit can help catch some of the errors. The business has been growing 20% a month and is now making more than $15 million in annual recurring revenue (ARR), according to Gill.\n--------------------\nInvestors find the startup’s growth exciting. On Tuesday, CodeRabbit announced that it raised a $60 million Series B, valuing the company at $550 million. The round, which brought the startup’s total funding to $88 million, was led by Scale Venture Partners with participation of NVentures, Nvidia’s venture capital arm, and returning investors including CRV.\n--------------------\nCodeRabbit is helping companies like Chegg, Groupon, and Mercury, along with over 8,000 other businesses, save time on the famously frustrating task of code review, which has become even more time-consuming with the rise of AI-generated code.\n--------------------\nSince CodeRabbit understands a company’s codebase, it can identify bugs and provide feedback, acting like a coworker, Gill said. He added that companies using CodeRabbit can cut the number of humans working on code-review by half.\n--------------------\nAs with most areas of AI, CodeRabbit has competition. Startup rivals include Graphite, which secured a $52 million Series B led by Accel earlier this year, and Greptile, which we reported is in talks for a $30 million Series A round with Benchmark.\n--------------------\nWhile leading AI coding assistants like Anthropic’s Claude Code and Cursor also offer AI-powered code review capabilities, Gill is betting that customers will prefer a standalone offering in the long term. “CodeRabbit is a lot more comprehensive in terms of depth and technical breadth than bundled solutions,” he said.\n--------------------\nWhether his prediction will turn out to be correct remains to be seen. But for now, thousands of developers are clearly happy to pay CodeRabbit $30 a month.\n--------------------\nEven with the growing popularity of AI code review tools like CodeRabbit, AI solutions still can’t yet be fully trusted to fix the bugs and “unusable” code written by AI. The unreliability of AI-generated code has given rise to a new corporate role: the vibe code cleanup specialist.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Gemini tops the App Store thanks to new AI image model, Nano Banana",
    "url": "https://techcrunch.com/2025/09/16/gemini-tops-the-app-store-thanks-to-new-ai-image-model-nano-banana/",
    "content": "Gemini’s mobile adoption has been soaring since the August launch of its Nano Banana image editor model, which has received positive reviews, particularly from users who say they can now more easily perform complex edits and create realistic images. The app has climbed to the top of global app stores’ charts and has seen a 45% month-over-month increase in downloads in the month of September so far, according to new data provided by app intelligence firm Appfigures.\n--------------------\nThough the month is only half over, Gemini’s app has already gained 12.6 million downloads in September, up from 8.7 million in August.\n--------------------\nBefore this month, Gemini had only gotten as high as No. 3 on the U.S. App Store on January 28, 2025.\n--------------------\nShortly after Nano Banana’s release, Gemini reached the No. 2 spot on the U.S. App Store on September 8. It then became the No. 1 app on September 12, where it has remained, after knocking OpenAI’s ChatGPT down to No. 2. No other dedicated AI apps are in the top 10 on the App Store at this time.\n--------------------\nGemini also became one of the top five iPhone apps overall in 108 countries globally, Appfigures’ data indicates.\n--------------------\nOn Google Play, Gemini jumped from the No. 26 overall top app in the U.S. on September 8 to become the No. 2 app as of Monday. However, despite Android being Google’s own platform, ChatGPT remains in the top spot as of the time of writing.\n--------------------\nGoogle has been touting Gemini’s growth, as more mainstream users have been trying out the new image-editing features. For instance, Google Gemini and Google Labs VP Josh Woodward shared on X on September 8 that the app had gained 23 million first-time users since the Nano Banana model launched, and those users had shared over 500 million images.\n--------------------\nThe app’s rapid growth is also driving increases in consumer spending.\n--------------------\nOf the $6.3 million Gemini generated this year on iOS devices, $1.6 million was from the month of August, with much of that coming in after the Nano Banana model’s release. That’s up 1,291% from January’s figure of $115,000, Appfigures estimates.\n--------------------\nThe app is also on track to at least match August’s number if not surpass it in September, as Gemini has pulled in $792,000 so far this month — roughly half of August’s total.\n--------------------\nThis year, Gemini’s app has been downloaded 103.7 million times and has seen 185.4 million downloads to date since its launch on Android in February 2024 and its expansion to iOS later that year.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Waymo’s Tekedra Mawakana on Scaling Self-Driving Beyond the Hype",
    "url": "https://techcrunch.com/2025/09/16/waymos-tekedra-mawakana-on-the-truth-behind-autonomous-vehicles-at-techcrunch-disrupt-2025/",
    "content": "Autonomous vehicles have long been touted as “just around the corner,” but the reality of bringing self-driving cars to the streets is far from simple. At TechCrunch Disrupt 2025 — October 27–29 at Moscone West in San Francisco — Waymo co-CEO Tekedra Mawakana joins the Disrupt Stage  for a wide-ranging conversation on the true state of AVs and where the industry goes from here.\n--------------------\nWhile headlines often highlight crashes, controversies, or overblown promises, Mawakana has spent years navigating the real-world path to autonomous mobility. At Disrupt, she’ll dig into what it actually takes to scale AV deployment — from rider safety and public trust to regulation, operations, and competitive pressure from Tesla and others.\n--------------------\nThis session isn’t about vague timelines or flashy demos. It’s about what’s working, what still needs work, and what it means to bring autonomy to life at scale. Whether you’re a founder, investor, or simply curious about the road ahead, you won’t want to miss it.\n--------------------\nTekedra Mawakana brings more than two decades of experience shaping global strategy at major tech companies. As co-CEO of Waymo, she guides the company’s mission to bring the Waymo Driver to the masses and lead the next generation of autonomous innovation. She also serves on Intuit’s board and advises several technology and social impact ventures.\n--------------------\nWaymo’s story is a central chapter in the future of transportation, and this session offers a rare inside look at the journey behind the headlines. Join us at TechCrunch Disrupt 2025, where 10,000+ startup and VC leaders gather to shape what’s next. Register today and save up to $650 before rates rise.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "OpenAI will apply new restrictions to ChatGPT users under 18",
    "url": "https://techcrunch.com/2025/09/16/openai-will-apply-new-restrictions-to-chatgpt-users-under-18/",
    "content": "OpenAI CEO Sam Altman announced on Tuesday a raft of new user policies, including a pledge to significantly change how ChatGPT interacts with users under the age of 18.\n--------------------\n“We prioritize safety ahead of privacy and freedom for teens,” the post reads. “This is a new and powerful technology, and we believe minors need significant protection.”\n--------------------\nThe changes for underage users deal specifically with conversations involving sexual topics or self-harm. Under the new policy, ChatGPT will be trained to no longer engage in “flirtatious talk” with underage users, and additional guardrails will be placed around discussions of suicide. If an underage user uses ChatGPT to imagine suicidal scenarios, the service will attempt to contact their parents or, in particularly severe cases, local police.\n--------------------\nSadly, these scenarios are not hypotheticals. OpenAI is currently facing a wrongful death lawsuit from the parents of Adam Raine, who died by suicide after months of interactions with ChatGPT. Character.AI, another consumer chatbot, is facing a similar lawsuit. While the risks are particularly urgent for underage users considering self-harm, the broader phenomenon of chatbot-fueled delusion has drawn widespread concern, particularly as consumer chatbots have become capable of more sustained and detailed interactions.\n--------------------\nAlong with the content-based restrictions, parents who register an underage user account will have the power to set “blackout hours” in which ChatGPT is not available, a feature that was not previously available.\n--------------------\nThe new ChatGPT policies come on the same day as a Senate Judiciary Committee hearing titled “Examining the Harm of AI Chatbots.” The hearing was announced by Sen. Josh Hawley (R-MO) in August. Adam Raine’s father is scheduled to speak at the hearing, among other guests.\n--------------------\nThe hearing will also focus on the findings of a Reuters investigation that unearthed policy documents apparently encouraging sexual conversations with underage users. Meta updated its chatbot policies in the wake of the report.\n--------------------\nSeparating underage users will be a significant technical challenge, and OpenAI detailed its approach in a separate blog post. The service is “building toward a long-term system to understand whether someone is over or under 18,” but in the many ambiguous cases, the system will default toward the more restrictive rules.\n--------------------\nFor concerned parents, the most reliable way to ensure an underage user is recognized is to link the teen’s account to an existing parent account. This also enables the system to directly alert parents when the teen user is believed to be in distress.\n--------------------\nBut in the same post, Altman emphasized OpenAI’s ongoing commitment to user privacy and giving adult users broad freedom in how they choose to interact with ChatGPT. “We realize that these principles are in conflict,” the post concludes, “and not everyone will agree with how we are resolving that conflict.”\n--------------------\nIf you or someone you know needs help, call 1-800-273-8255 for the National Suicide Prevention Lifeline. You can also text HOME to 741-741 for free; text 988; or get 24-hour support from the Crisis Text Line. Outside of the U.S., please visit the International Association for Suicide Prevention for a database of resources.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "D-ID acquires Berlin-based video startup Simpleshow",
    "url": "https://techcrunch.com/2025/09/16/d-id-acquires-berlin-based-video-startup-simpleshow/",
    "content": "Video generation and editing platform D-ID said Tuesday that it has acquired Berlin-based B2B video creation platform Simpleshow. The companies didn’t disclose financial terms of the deal.\n--------------------\nSimpleshow’s product will operate under D-ID’s umbrella, and eventually the two platforms will merge, D-ID chief executive Gil Perry told TechCrunch.\n--------------------\nSimpleshow, founded in 2008, has raised over $20 million in funding, according to Crunchbase data.\n--------------------\nThe startup has offices in Berlin, Luxembourg, London, Miami, Singapore, Hong Kong, and Tokyo. As part of the merger, the company will have consolidated offices in Berlin, Tel Aviv, and the United States. D-ID didn’t mention Simpleshow’s team size but said that the combined entity will have 140 employees.\n--------------------\n“Simpleshow initially approached us for a strategic partnership. We saw that there was synergy between management teams and products,” said Perry. “We felt that we needed to increase our speed in capturing a large [part of the enterprise avatar video] market. We thought acquiring Simpleshow would give us the necessary boost in that.”\n--------------------\nBoth companies are seeing a strong future of digital avatars for different kinds of videos, including training, marketing, and sales. D-ID already has a suite of AI-powered interactive avatars that it offers to its clients.\n--------------------\nSimpleshow’s CEO Karsten Boehrs said that when he joined the company over a decade ago, it was largely an agency producing videos for businesses and enterprises.\n--------------------\n“To achieve scale and serve more clients internationally, we decided to build a SaaS-based tech platform,” Boehrs told TechCrunch. “One of the first tools we launched was a text-to-video tool for our clients in 2017.”\n--------------------\nBoehrs added that in the last few years, with the rise of AI, it started conversations with companies like Synthesia for potential partnerships and eventually landed on D-ID to get acquired.\n--------------------\nAlongside its product, Simpleshow is also bringing more than 1,500 enterprise clients, including Adobe, Audio, Airbus, Microsoft, Bayer, HP, T-Mobile, McDonald’s, eBay, and Deutsche Bank. D-ID’s Perry mentioned that this will boost the company’s bottom line and bring it closer to profitability.\n--------------------\nGoing forward, D-ID wants to build interactive training videos, which will let users interrupt a video presented by an avatar and ask them a question or take a quiz.\n--------------------\nD-ID has strong competition for enterprise adoption of digital avatars in companies like Synthesia and Soul Machines. Companies such as Google and McKinsey are also developing solutions to let clients use digital avatars.\n--------------------\nD-ID has raised $60 million in funding to date. The company said it has secured funding to bankroll the acquisition, but it didn’t disclose the money.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "YouTube to use AI to help podcasters promote themselves with clips and Shorts",
    "url": "https://techcrunch.com/2025/09/16/youtube-to-use-ai-to-help-podcasters-promote-themselves-with-clips-and-shorts/",
    "content": "On Tuesday, YouTube introduced new tools for podcasters at its Made on YouTube live event in New York, including new ways to turn video podcasts into clips and YouTube Shorts and a new feature that helps create videos for audio-only podcasters. Both will be powered by AI and will roll out in the months ahead.\n--------------------\nUsing AI technology, video podcast creators in the U.S. will be able to create clips more easily with AI suggestions, the company says. This will be available in the “coming months,” while a feature that will transform those clips into YouTube Shorts will arrive early next year.\n--------------------\nThe addition could give YouTube more fodder to compete with rival short-form video apps, like TikTok and Instagram (Reels), while also directing users to podcasters they might find interesting on YouTube’s larger platform, driving subscriptions and engagement.\n--------------------\nMeanwhile, another new feature also available early next year will help audio podcasters turn their content into video. Using AI, these creators will be able to generate a customizable video for their podcast, the company says. However, the feature will only be available to “select podcasters” when it launches, with a larger expansion planned for later in 2026.\n--------------------\nYouTube has been more focused on building out tools for podcasters over the past several years, making pods a more prominent feature on YouTube’s home page and its YouTube Music service. Meanwhile, Spotify has been inching into its market with added support for video podcasts and other engagement features for podcasters, like comments, polls, and Q&As, as well as monetization tools.\n--------------------\nIn February, the company announced YouTube had surpassed 1 billion monthly podcaster viewers. Today, YouTube announced that users, as of July 2025, now consume over 100 million hours of podcasts daily, with more than 30% of those hours starting as a livestream or premiere.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "YouTube announces new generative AI tools for Shorts creators",
    "url": "https://techcrunch.com/2025/09/16/youtube-announces-new-generative-ai-tools-for-shorts-creators/",
    "content": "At its Made on YouTube live event on Tuesday, the company unveiled new generative AI tools for Shorts creators. YouTube is bringing a custom version of Google’s text-to-video generative AI model, Veo 3, to Shorts, along with a new remixing tool, an “Edit with AI” feature, and more.\n--------------------\nThe custom version of Veo 3, called Veo 3 Fast, generates outputs with lower latency at 480p, making it easy to create video clips, YouTube says. And now users can do so with sound for the first time.\n--------------------\nThis update is rolling out in the United States, the United Kingdom, Canada, Australia, and New Zealand. YouTube plans to expand its functionality to more regions in the future.\n--------------------\nYouTube is also bringing new Veo capabilities to Shorts, including the ability to apply motion from a video to an image. For example, you could animate a still image by making the person in it do a dance from a video. The company says this is possible through technology that captures and transfers movement from one subject to another.\n--------------------\nCreators can now also use Veo to apply different styles to their videos, such as pop art or origami. Plus, creators now have the ability to add objects like characters or props with text descriptions.\n--------------------\nThese new capabilities will roll out in the coming months.\n--------------------\nAs for the new remixing tool, creators can turn the dialogue from eligible videos into catchy soundtracks for other Shorts.\n--------------------\n“As the world’s largest creative playground, YouTube is where trends are born and where you can draw inspiration from. Imagine hearing a line of dialogue that sparks an idea — a funny phrase, a memorable quote, or a one-of-a-kind sound — and you want to remix it into a new sound,” YouTube’s Director of Product, Shorts and Generative AI Creation, Dina Berrada, wrote in a blog post. “With our new Speech to Song remixing tool, you’ll be able to do just that.”\n--------------------\nYouTube notes that the feature uses Google’s AI music model Lyria 2 to create the soundtrack. Creators will be able to add their own vibe to the song, like “chill,” “danceable,” or “fun.”\n--------------------\nThe company plans to test this feature soon, it says, and will roll it out to more creators in the United States in the coming weeks.\n--------------------\nWith the new Edit with AI feature, creators can turn their raw footage into first drafts. It transforms raw camera roll footage into a first draft by finding and arranging the best moments and adding music and transitions. It can even add a voice-over that can react to what’s happening in the video, in either English or Hindi. The idea behind the feature is to give creators a starting point for their Shorts, YouTube says.\n--------------------\nYouTube is experimenting with Edit with AI on Shorts and in the YouTube Create app and will expand the feature in the coming weeks in select markets.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "YouTube rolls out Studio updates, ‘likeness’ detection, lip-synced dubs, creator collabs, and more",
    "url": "https://techcrunch.com/2025/09/16/youtube-rolls-out-studio-updates-likeness-detection-lip-synced-dubs-creator-collabs-and-more/",
    "content": "YouTube on Tuesday announced a suite of new features coming to YouTube Studio, the platform over 30 million creators use to manage their channels and track their analytics and revenue every month. At its Made on YouTube event, the company unveiled new and updated tools like an AI-powered chatbot for support, an inspiration tab, title A/B testing features, auto dubbing, likeness detection tools, and more.\n--------------------\nMany of these features build on tools previously announced or tested with smaller groups but are now rolling out more broadly.\n--------------------\nOf these, the most interesting addition is the likeness-detection feature, which was first announced in 2024 and expanded earlier this year to a handful of top creators, like MrBeast. Now the company says it’s bringing the technology to an open beta that will be available to all YouTube Partner Program creators — content creators who meet certain subscriber and view thresholds to monetize their channels. These creators will be able to detect, manage, and authorize the removal of any unauthorized videos using their facial likeness. This will help them protect their image and reputation, and ensure their audience isn’t misled, notes YouTube.\n--------------------\nAnother new tool, Ask Studio, provides an AI-powered chatbot assistant that can guide users and answer questions about their account, like how their latest video is performing or what their audience is saying about their editing style, for example. The tool is meant to offer creators actionable insights that will help them grow their channel, according to YouTube.\n--------------------\n(The feature is different from another “Ask” AI tool for viewers that YouTube tested in late 2023, which allowed users to ask questions about a video they were viewing.)\n--------------------\nOne feature getting an update is the Inspiration tab in YouTube Studio. Launched publicly at last year’s event, the tab helps creators leverage AI to spark ideas and come up with video concepts. Now it’s being updated with new ways to generate ideas, including a list of suggested topics tailored to each creator’s channel and a set of nine responses to every AI prompt, to help creators build out their content plan. The company notes that the topics can be combined, or users can add their own, as they brainstorm. The feature will also explain why it’s making specific suggestions based on audience insights and behavior.\n--------------------\nYouTube Studio will also introduce a way to test and compare up to three different video titles and thumbnails, as an update to its A/B testing feature launched to select creators in 2023 and expanded the following year. Creators have used this testing feature more than 15 million times so far, according to the company (a metric that seems a bit small, given that 20 million videos are uploaded to the site daily).\n--------------------\nPlus, creators will be able to collaborate with up to five others on one video that is shown to the audiences of all the participating creators. While the feature is aimed at boosting engagement and helping creators reach new viewers, the revenue earned from the video will be attributed to the channel that posts the video, YouTube says.\n--------------------\nThe company says it will also begin testing lip-syncing technology to make its auto dubbing features more realistic. Today, YouTube supports dubbing content into 20 different languages, and in the coming months, it will improve the translated videos to make them appear more natural by matching lip movements to the dubbed audio.\n--------------------\nYouTube notes that, on average, viewers spent over 75% of their time viewing the autodubbed video compared to the original, based on a comparison that ran from December 2024 to August 2025.\n--------------------\n\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Salesforce launches ‘Missionforce,’ a national security-focused business unit",
    "url": "https://techcrunch.com/2025/09/16/salesforce-launches-missionforce-a-national-security-focused-business-unit/",
    "content": "Salesforce is increasing its focus on national security.\n--------------------\nThe customer relationship management giant announced the creation of a new business unit called Missionforce on Tuesday. It will be focused on incorporating AI into defense workflows in three main areas: personnel, logistics, and decision-making, according to a company press release.\n--------------------\nMissionforce will be helmed by Kendall Collins, the CEO of Government Cloud. Collins joined Salesforce in 2023 and was previously the chief business officer and chief of staff to Salesforce CEO Marc Benioff.\n--------------------\n“With Missionforce, we’ll now bring the best of AI, cloud, and platform technology from the private sector to modernize critical areas including personnel, logistics, and analytics,” Collins said in the press release. “The goal is simple: to help our warfighters and the organizations that support them operate smarter, faster, and more efficiently. There’s never been a more important time to serve those who serve.”\n--------------------\nSalesforce has held contracts with the U.S. government for years across federal agencies and multiple branches of the U.S. military, including the U.S. Army, Navy, and Air Force. The company doesn’t publicly disclose how many government contracts it has nor how much revenue it makes from them.\n--------------------\nThis news is the latest in a wave of tech companies building and offering services specifically for the U.S. government.\n--------------------\nOpenAI launched a version of its ChatGPT designed for U.S. government agencies in January. In August, the company announced it struck a deal with the government to give federal agencies access to its enterprise ChatGPT tier for just $1 a year.\n--------------------\nOther companies quickly followed suit.\n--------------------\nA week later, Anthropic announced it was giving the U.S. government access to its government and enterprise tiers of its Claude chatbot for $1.\n--------------------\nGoogle announced “Gemini for Government” in late August, which offers their AI services to federal agencies for 47 cents for the first year.\n--------------------\nThis piece was updated to better reflect Kendall Collins position.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Google launches new protocol for agent-driven purchases",
    "url": "https://techcrunch.com/2025/09/16/google-launches-new-protocol-for-agent-driven-purchases/",
    "content": "On Tuesday, Google announced a new open protocol for purchases initiated by AI agents — automated software programs that can shop and make decisions on behalf of users —  with backing from more than 60 merchants and financial institutions. Called the Agent Payments Protocol (AP2), the system is meant to be interoperable between AI platforms, payment systems and vendors, providing a traceable paper trail for each transaction.\n--------------------\nIn a post announcing the protocol, Google executives emphasized their commitment to openness. “We are committed to evolving this protocol in an open, collaborative process, including through standards bodies, and  invite the entire payments and technology community to build this future with us,” wrote Stavan Parikh and Rao Surapaneni, who are VPs at Google and Google Cloud, respectively.\n--------------------\nThe full specification for AP2 was posted to GitHub in conjunction with the announcement.\n--------------------\nThe protocol is built for a future in which AI agents routinely shop for products on customers’ behalf and engage in complex real-time interactions with retailers’ AI agents. One example in Google’s post imagines a chatbot user asking their agent to shop for a bike trip, which triggers a spontaneous time-sensitive bundle offer from a bike shop’s agent.\n--------------------\nIn another example, a user asks for travel and lodging for a weekend vacation, giving only the dates, location, and budget. “The agent can then interact with both airline and hotel agents, as well as online travel agencies and booking platforms,” the post explains, “and once it finds a combination that fits the budget, it can execute both cryptographically-signed bookings simultaneously.”\n--------------------\nEnabling that kind of transaction is complex, from both a technological and social standpoint. AP2 requires agents to register two separate approvals before a purchase can be made: first the “intent mandate” (essentially telling the AI, “I’m looking for a polka-dot necktie”), which enables the agent to search for a specific item and negotiate with sellers; then the “cart mandate,” which gives final approval for a purchase once a specific item has been found.\n--------------------\nThe protocol also contains a provision for fully automated purchases, in which the agent is permitted to automatically generate a cart mandate once an item is found. Those circumstances require a more detailed intent mandate, specifying price limits, timing, and other rules of engagement. In either case, the goal is to maintain an auditable trail that can be reexamined in cases of fraud.\n--------------------\nIn collaboration with cryptocurrency outfits Coinbase, MetaMask, and the Ethereum foundation, Google also produced an extension that would integrate the cryptocurrency-oriented x402 protocol, allowing for AI-driven purchasing from crypto wallets.\n--------------------\nA number of other tech companies are working on their own agentic purchasing systems — most notably Perplexity, which allows for a Buy With Pro service in its agentic browser. The payment provider Stripe also produces software tools for agentic purchasing on its platform, though they are not as comprehensive as AP2.\n--------------------\nLike any protocol, the impact of AP2 will depend on its support from other players in the ecosystem — most notably, developers building agentic purchasing systems. But AP2 has already won the support of major financial providers like Mastercard, American Express, and PayPal, giving the protocol a significant immediate footprint.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "This $30M startup built a dog crate-sized robot factory that learns by watching humans",
    "url": "https://techcrunch.com/2025/09/16/this-30m-startup-built-a-dog-crate-sized-robot-factory-that-learns-by-watching-humans/",
    "content": "While many robotics companies are building human-sized robots, or working to automate entire factories, MicroFactory is instead trying to think big by building small.\n--------------------\nSan Francisco-based MicroFactory built a general-purpose, tabletop manufacturing kit that’s about the size of my Siberian husky’s dog crate. This compact factory includes two robotic arms and can be trained by human demonstration, as well as through AI.\n--------------------\n“General purpose robots are good, but it’s not necessary [to] be humanoid,” said Igor Kulakov, the co-founder and CEO of MicroFactory, in an interview with TechCrunch. “We decided to design robots from scratch that will still be general purpose but not in human shape, and this way, it can be done much simpler, much easier, in hardware and on the AI side.”\n--------------------\nRather than selling individual robotic arms, MicroFactory’s system comes as an enclosed but transparent workstation, allowing users to watch the manufacturing process in real time. The compact factory-in-a-box is designed for precision tasks like circuit board assembly, component soldering, and cable routing. Users can train the robots by physically guiding the arms through complex motions — a hands-on approach that Kulakov says works faster than traditional AI programming for intricate manufacturing sequences.\n--------------------\n“Usually it takes couple hours, but in this way, the robot much better understands what it should do,” Kulakov said. “When you hire people, we still need to spend time, like a week or something, to instruct these people to then supervise their work. A manufacturing company, they already have this time and resources to spend, and it will be much easier to train a model and to make it work in this way.”\n--------------------\nKulakov’s experience with traditional manufacturing helped spark the idea behind MicroFactory.\n--------------------\nHe and his co-founder, Viktor Petrenko, used to run bitLighter, a manufacturing business that made portable lighting equipment for photographers. Kulakov said it was difficult to train new employees on how to complete the manufacturing process correctly. When advancements in AI made it seem possible to automate this type of work, they decided to jump on the opportunity.\n--------------------\nKulakov and Petrenko launched MicroFactory in 2024. It took them about five months to build their prototype. Now the company has hundreds of preorders from customers looking to use the machines for various applications, including assembling electronics and even processing snails to be shipped to France for escargot.\n--------------------\nMicroFactory just raised a $1.5 million pre-seed funding round that included investors like executives from the AI company Hugging Face and investor-entrepreneur Naval Ravikant. The round values the young startup at a $30 million post-money valuation.\n--------------------\nKulakov said the company plans to use the funding to build and ship out its units. The company is currently converting its prototype into a commercial product that it hopes to begin shipping in about two months.\n--------------------\nThe company also plans to make some hires and continue improving its technology, including the AI models running under the hood.\n--------------------\n“Our growth is related to building hardware, so we set the goal to increase it 10x each year,” Kulakov said. “In the first year, we want to produce 1,000 robots, [about] three per day, and we have the capability to do this. Then, [we want to] make more and more productions.”\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Y Combinator-backed Rulebase wants to be the AI co-worker for fintech",
    "url": "https://techcrunch.com/2025/09/16/y-combinator-backed-rulebase-wants-to-be-the-ai-co-worker-for-fintech/",
    "content": "Y Combinator-alum Rulebase is betting that the next wave of automation in financial services won’t be about flashy AI interfaces but rather the unglamorous back-office tasks like compliance.\n--------------------\nThe startup, founded in 2024 by Gideon Ebose and Chidi Williams, two Nigerian engineers who met in London, just raised a $2.1 million pre-seed round led by Bowery Capital, with participation from Y Combinator, Commerce Ventures, Transpose Platform VC, and several angels.\n--------------------\nFinancial services firms spend enormous amounts of effort on support tickets, resolving disputes, ensuring quality assurance, and regulatory compliance. Rulebase’s software, which it calls an agent co-worker, replaces much of the manual grunt work in these tasks. Its AI agent can evaluate customer interactions, flag regulatory risks, and trigger the right follow-ups across tools like Zendesk, Jira, and Slack without losing the human-in-the-loop oversight that financial firms demand.\n--------------------\n“Our ‘Coworker’ tool integrates across platforms and collaborates with human agents and back-office teams to fully manage the dispute life cycle while saving time, reducing errors, and maintaining compliance,” said CTO Williams. Currently, the year-old startup is already deployed at customers like U.S. business banking platform Rho and an unnamed Fortune 50 financial institution.\n--------------------\nRulebase wasn’t the founders’ first swing. Ebose, a former product lead at Microsoft, and Williams, a former back-end engineer at Goldman Sachs, built several products together like an AI customer feedback tool before eventually settling on Rulebase. The idea came about after seeing how inefficient back-office operations were in small and large financial institutions, especially when it came to regulatory workflows.\n--------------------\nThe startup currently focuses on workflows triggered by customer service interactions, with its first wedge around quality assurance. QA analysts in traditional financial institutions typically manually review 3% to 5% of support interactions to ensure reps follow compliance protocols.\n--------------------\nRulebase now evaluates 100% of such interactions, cutting costs by up to 70%, the founders say. In the case of Rho, for instance, Rulebase has helped cut escalations by up to 30%.\n--------------------\n“We automate workflows that start with a customer interaction, areas we’re already great at handling end-to-end,” CEO Ebose said in an interview with TechCrunch. “While much of that is QA, compliance, and disputes tied to customer calls and messages, long-term our goal is to take on as many manual back-office tasks as possible by pulling these fragmented steps and tabs into one coordinated workflow.”\n--------------------\nThe new funding will help it double down on engineering and eventually add new features to AI Coworker like fraud investigation, audit preparation, and regulatory reporting.\n--------------------\nRulebase is focused on financial services for now because automation demands precision. “You need to understand Mastercard’s rules, CFPB timelines. That depth of domain knowledge is our moat,” Ebose said.\n--------------------\nThe company is targeting business banks, neobanks, and card issuers across Africa, Europe, and the U.S. But the roadmap could eventually include adjacent verticals like insurance, where similar workflows exist.\n--------------------\nRevenue is growing fast, with “double-digit” month-over-month growth since joining Y Combinator’s Fall 2024 batch, the founders say. Rulebase’s business model is usage-based, charging per interaction reviewed or workflow automated.\n--------------------\nAs one of the few African founders to get into YC building AI tools, Ebose and Williams’ advice to founders trying to get accepted into the global accelerator is to think globally from day one.\n--------------------\n“We’re in a moment where small teams can deliver more value, more quickly, than ever before, so limiting yourself to ‘X for Y’ or a narrow vertical feels like a missed opportunity,” Williams remarked. “With AI, it feels obvious that you have to go after something massive. Anything less than the most ambitious version of your idea likely won’t cut it,” said Williams, who before Rulebase built Buzz, an early open source speech-to-text tool with over 300,000 downloads and over 12,000 GitHub stars.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "The 9 most sought-after startups from YC Demo Day",
    "url": "https://techcrunch.com/2025/09/15/the-9-most-sought-after-startups-from-yc-demo-day/",
    "content": "Y Combinator hosted its Summer 2025 Demo Day last week, showcasing the latest batch of over 160 startups.\n--------------------\nAs with recent batches, the majority of startups presented AI-centric solutions. However, a clear evolution was evident. Instead of  “AI-powered” products, many companies are now building AI agents or the infrastructure and tools needed to develop them. For instance, this batch had a flurry of voice AI solutions and new businesses focused on helping others monetize the “AI economy” with ads and marketing tools.\n--------------------\nWe spoke with a handful of YC-focused  investors to learn which startups they found most interesting and which generated the highest investment demand.\n--------------------\nBelow are the most frequently mentioned ones:\n--------------------\nWhat it does: Stripe for AI startups\n--------------------\nWhy it’s a fave: Many AI startups use complex pricing models that often blend a flat subscription fee per seat with usage-based charges, credits, and various add-on costs. Managing complex AI pricing on Stripe is a time-consuming, manual process. That’s why Autumn developed an open-source infrastructure that simplifies Stripe integration for AI startups. The company says its technology is already used by hundreds of AI apps and 40 YC startups. Given Stripe’s dominance in payments and the explosive growth of the AI market, could a specialized billing solution for AI be the next major fintech success story?\n--------------------\n\n--------------------\nWhat it does: Builds Vercel for AI agents\n--------------------\nWhy it’s a fave: Just as Vercel helps developers deploy and host startups, Dedalus Labs claims its platform automates the infrastructure for building AI agents, cutting hours of coding down to a few clicks. The company handles complex tasks like autoscaling and load balancing, which it says makes agent deployment fast and simple.\n--------------------\nWhat it does: crowdsource rankings of vibe coded designs\n--------------------\nWhy it’s a fave: The ability of AI to rapidly generate a huge number of designs creates a new problem: figuring out which ones are actually good. Design Arena solves this by crowdsourcing rankings of AI-generated visuals, creating a feedback loop that forces AI models to improve. Large AI labs see value in training their models to generate better designs, as some of them are already Design Arena’s customers.\n--------------------\nWhat it does: Tech-enabled distributor for retailers in Southeast Asia\n--------------------\nWhy it’s a fave: Getasap Asia was founded by Raghav Arora three years ago when he was just 14 years old. Since then, the startup that uses tech to deliver supplies to corner stores, restaurants and large supermarkets in Southeast Asia in under eight hours, has earned millions in revenue. Getasap Asia closed a round from General Catalyst, according to its website, and we are hearing that the startup’s valuation was among the highest in the whole batch.\n--------------------\nWhat it does: AI engineer that fixes bugs in production\n--------------------\nWhy it’s a fave: Founded by a 20-year-old Pablo Hansen who last year earned a master’s degree in AI, Keystone is on a mission to reduce software breaks. The company’s AI finds and fixes bugs for clients like Lovable and has already turned down a seven-figure acquisition offer, Hansen said.\n--------------------\nWhat it does: AI matchmaker for female friends\n--------------------\nWhy it’s a fave: While there isn’t a shortage of dating apps, RealRoots is tackling a different kind of loneliness. The company’s AI matchmaker, Lisa, interviews women and then organizes social experiences to connect them with compatible friends. While the AI part might be performative –  conversations with Lisa probably wouldn’t give RealRoots more insights about participants than written answers would – RealRoots may be on to something. Last month alone, the company generated $782,000 from 9,000 paying clients, its founders said.\n--------------------\nWhat it does: Automates insurance claims with AI\n--------------------\nWhy it’s a fave: Solva’s AI automates the most routine tasks for insurance adjusters, from filling out complex claims to preventing improper payouts. Just ten weeks after launching, Solva has already amassed $245,000 in annual recurring revenue (ARR), a figure that has investors excited.\n--------------------\nWhat it does: counter-drone mini-missiles\n--------------------\nWhy it’s a fave: With China reportedly amassing swarms of inexpensive drones, the U.S. military faces an urgent need for cost-effective counter-drone solutions. Perseus is developing just that: small missiles designed to shoot down drones at a fraction of the cost of existing systems. Multiple branches of the U.S. military have already invited the startup to demonstrate its solution, which could lead to hefty contracts.\n--------------------\nWhat it does: AI foreign language tutor\n--------------------\nWhy it’s a fave: Apps like Duolingo have made language learning accessible and fun, but they often lack a key component of fluency: consistent conversation. Pingo solves this problem by allowing users to speak with its AI, which acts as a native speaker. The company’s unique approach is proving incredibly popular, with founders claiming it’s growing 70% monthly and earning $250,000 in monthly revenue.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Inside the shift at Disrupt: Building community and scaling in the AI era",
    "url": "https://techcrunch.com/2025/09/15/on-stage-at-techcrunch-disrupt-2025-how-ai-is-forcing-late-stage-startups-to-rewire-gtm-or-be-left-behind/",
    "content": "At TechCrunch Disrupt 2025, taking place October 27–29 in San Francisco, industry leaders will explore how founders can build lasting companies while navigating rapid shifts in AI, markets, and consumer behavior. Catch this dynamic panel on the Going Public Stage, where three seasoned leaders will explore how AI is transforming go-to-market (GTM) strategies from the inside out.\n--------------------\nBuy your tickets now to join this conversation — and more than 200 others — at the premier gathering for startups and investors. Register now for TechCrunch Disrupt 2025 and save up to $668, available through September 26. Prices increase September 27.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "OpenAI upgrades Codex with a new version of GPT-5",
    "url": "https://techcrunch.com/2025/09/15/openai-upgrades-codex-with-a-new-version-of-gpt-5/",
    "content": "OpenAI announced Monday that it’s releasing a new version of GPT-5 to its AI coding agent, Codex. The company says its new model, called GPT-5-Codex, spends its “thinking” time more dynamically than previous models and could spend anywhere from a few seconds to seven hours on a coding task. As a result, it performs better on agentic coding benchmarks.\n--------------------\nThe new model is now rolling out in Codex products — which can be accessed via a terminal, IDE, GitHub, or ChatGPT — to all ChatGPT Plus, Pro, Business, Edu, and Enterprise users. OpenAI says it plans to make the model available to API customers in the future.\n--------------------\nThe update is part of OpenAI’s effort to make Codex more competitive with other AI coding products, such as Claude Code, Anysphere’s Cursor, or Microsoft’s GitHub Copilot. The market for AI coding tools has become much more crowded in the last year as a result of intense user demand. Cursor surpassed $500 million in ARR earlier in 2025, and Windsurf, a similar code editor, was the subject of a chaotic acquisition attempt that saw its team split between Google and Cognition.\n--------------------\nOpenAI says that GPT-5-Codex outperforms GPT-5 on SWE-bench Verified, a benchmark measuring agentic coding abilities, as well as a benchmark measuring performance on code refactoring tasks from large, established repositories.\n--------------------\nThe company also says it trained GPT-5-Codex for conducting code reviews and asked experience software engineers to evaluate the model’s review comments. The engineers reportedly found GPT-5-Codex to submit fewer incorrect comments, while adding more “high-impact comments.”\n--------------------\nIn a briefing, OpenAI’s Codex product lead Alexander Embiricos said that much of the increased performance was thanks to GPT-5-Codex’s dynamic “thinking abilities.” Users may be familiar with GPT-5’s router in ChatGPT, which directs queries to different models based on the complexity of a task. Embiricos said GPT-5-Codex works similarly but has no router under the hood and can adjust for how long to work on a task in real time.\n--------------------\nEmbiricos says this is an advantage compared to a router, which decides how much computational power and time to use on a problem at the outset. Instead, GPT-5-Codex can decide five minutes into a problem that it needs to spend another hour. Embiricos said he’s seen the model take upward of seven hours in some cases.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "China says Nvidia violated antitrust regulations",
    "url": "https://techcrunch.com/2025/09/15/china-says-nvidia-violated-antitrust-regulations/",
    "content": "Trade tensions between China and the U.S. regarding semiconductors just got even more strained.\n--------------------\nOn Monday, China’s State Administration for Market Regulation ruled that semiconductor giant Nvidia was in violation of the country’s antitrust regulations, as first reported by Bloomberg. The ruling was in reference to Nvidia’s 2020 acquisition of Mellanox Technologies, a computer networking supplier, for $7 billion.\n--------------------\nAn Nvidia spokesperson supplied the following statement: “We comply with the law in all respects. We will continue to cooperate with all relevant government agencies as they evaluate the impact of export controls on competition in the commercial markets.”\n--------------------\nChina didn’t announce any consequences tied to its findings and will continue to investigate. Still, the ruling is likely to cast a pall over ongoing tariff negotiations between the U.S. and China, currently taking place in Madrid. While these trade discussions aren’t specifically about semiconductors, the question of Chinese access to Nvidia chips is a major point of contention between the two regimes.\n--------------------\nThe outgoing Biden administration announced its AI Diffusion Rule back in January that was meant to restrict U.S.-made AI chips to many countries, with further restrictions specifically for China and other adversaries.\n--------------------\nWhile the U.S. Department of Commerce formally repealed Biden’s AI rule in May, the future of AI chip exports to China remains in flux. The Trump administration slapped licensing agreements on chips heading to China in April. A few months later, in July, these companies were given the green light to start selling these chips again.\n--------------------\nJust a few weeks after that the country struck a deal requiring companies selling chips to China to give the U.S. a 15% cut of the revenue made on those sales. China has discouraged firms from buying Nvidia chips and, as of a recent earnings call, none of the company’s chips have made it through the new export process.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Apple’s iOS 26 with the new Liquid Glass design is now available to everyone",
    "url": "https://techcrunch.com/2025/09/15/apples-ios-26-with-the-new-liquid-glass-design-is-now-available-to-everyone/",
    "content": "Apple’s iOS 26 software update for iPhones is available Monday to people who have an iPhone 11, iPhone SE 2, and later. The marquee feature of iOS 26 is Apple’s Liquid Glass design, which includes elements on-screen that resembled a “glassy” look. Other features include a call screening assistant, a new gaming and preview app, in-app translation across the system, and updates to Genmoji and Image Playground apps.\n--------------------\nThe operating system update also went through a big numerical change as Apple jumped from iOS 18 to iOS 26 for two key reasons. First, it wanted to bring all operating systems — iOS, iPadOS, macOS, watchOS, tvOS, and VisionOS — in sync. And it also wanted to reflect the year number in which the majority of the people will use this update.\n--------------------\nLiquid Glass design has been the most significant visual overhaul for iOS in years. Apple’s intention with this redesign was to take inspiration from the Vision Pro interface and apply it to all of its operating systems. The elements are meant to look like they are made of translucent glass. This resulted in challenges in terms of readability and how elements in the background look.\n--------------------\nSince June, Apple has made several changes to how “glassy” the interface looks through beta releases. While the company is releasing the stable version of iOS 26 today, we might expect visual tweaks for improved readability and usability in the coming months. This visual change might take a bit of time for users to adjust, and they might not like certain elements right away.\n--------------------\nThe Phone app has a new unified look where your favorites are up top in a card format with recents and voicemails on the same screen. You can tap the filter button on the top right and look at these sections individually as well. (If you don’t like the new interface, Apple also gives you an option to switch to the classic look.)\n--------------------\nIn addition, iOS 26 brings a call-screening feature to iPhones, which is a personal favorite. When an unknown number calls you, the system asks for their name and the purpose of the call. Once they give this information, the system invokes the ringer and notifies you of the call. You can look at the conversation and interject at any time. Transcription of voicemail doesn’t work well for all languages, but call screening has reduced the number of calls I’ve had to pick up.\n--------------------\nThere’s also a holding assist for when a restaurant or a helpline places you on hold; you can use call assist to notify when an agent starts talking again.\n--------------------\nThe Messages app is getting to feature party with other chat apps like WhatsApp and Telegram with backgrounds, new conversation flow, polls, text selection, photo previews, and typing indicators in groups. Apple has been working on SMS filtering for a few years now. The company said it updated its spam filtering with this release. Plus, it places messages from unknown senders in a new folder. One thing I didn’t like about this update is that it takes me a couple of taps to go to the transactions tab.\n--------------------\nThe games app overhaul means that you can look at the games you are playing (or have played), arcade games, challenges, and achievements in one place, along with suggestions for new titles. The app also shows you what your friends are playing.\n--------------------\nApple finally added Mac’s Preview app to iOS 26, which means you can edit, annotate, and sign PDFs more easily.\n--------------------\nMeanwhile, Apple Music now has automixing for dynamic song switching, along with lyrics translation and pronunciation. What’s more, you can pin your favorite songs and playlists.\n--------------------\nWith iOS 26, Apple Maps lets you define preferred routes while commuting. In case your choice of route has more traffic or any incidents, Maps sends you a notification along with suggesting alternative routes. The app also lets you easily view visited places through a new places library.\n--------------------\nThe Camera app in iOS 26 adopts the Liquid Glass design with only Video and Photo options visible by default. You can scroll to the left or right to switch between different modes. Apple has placed some controls like Flash and Night mode on the top right, and you can switch them on/off with one touch. For more options like filters, styles, exposure controls, and timer, you can swipe up from the bottom of the screen.\n--------------------\nIf you didn’t like the previous Photos app design, the tabs are back in this version.\n--------------------\nUnlike last year’s grand launch of Apple Intelligence, this year’s operating system is light on AI features, especially given delays in launching and rolling out features. The company is making AI-powered translation easily available in apps like Messages, FaceTime, and Phone. Currently, this feature supports English (U.K., U.S.), French (France), German, Portuguese (Brazil), and Spanish (Spain).\n--------------------\nThrough iOS 26, the company is also launching live translation on AirPods, including the newly launched AirPods Pro 3, AirPods Pro 2, and AirPods 4.\n--------------------\nAlso, iOS 26 updates visual intelligence to understand the content on the screen. You have to press Power + the volume down button to bring up this menu. Apple Intelligence can then suggest events to add to your calendar. You can also ask questions about the content on the screen, using Google Visual Search or ChatGPT. Apple is also releasing its own “Circle to search” called Highlight.\n--------------------\nThe most confusing part about this update is that the buttons used to bring up on-screen visual intelligence are the same as the screenshot button. Because of this, it takes an extra step to save a screenshot, and I have forgotten to save some important screenshots.\n--------------------\nApple is updating Genmoji with iOS 26 to let you merge two emojis with a text prompt and make something new. You can now add expression to people in both Genmoji and Image Playground. Update to Image Playground now allows you to modify attributes like hair and facial hair, along with new styles from ChatGPT.\n--------------------\nWe have a list of tons of small but useful iOS features here. You can update to iOS 26 by going to Settings > General > Software Update and downloading the latest version.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Spotify will now let free users pick and play tracks",
    "url": "https://techcrunch.com/2025/09/15/spotify-will-now-let-free-users-pick-and-play-tracks/",
    "content": "Following its long-awaited launch of lossless streaming for paid subscribers, Spotify is upgrading its service for free users, too. On Monday, the company announced that free users globally will now be able to search and play any song they want or play a song shared by a friend or an artist they follow on social media.\n--------------------\nThe company calls the new features “Pick & Play,” “Search & Play,” and “Share & Play,” respectively. With the former, free users can hit play in the Spotify app to pick and play any song they want, or can even search for a particular song and play it.\n--------------------\nThe “Share & Play” feature could encourage free users to open Spotify when they come across music on social media. For instance, Instagram lets users share a Spotify track to Stories with sound and allows users to share music on Instagram Notes.\n--------------------\nPreviously, free users could shuffle songs with limited skips on mobile devices.\n--------------------\nSpotify says the new features will roll out globally to free users, but there will still be some restrictions that Premium users won’t face.\n--------------------\nWhen reached for clarification, the company told TechCrunch that users on the free mobile experience will have an allocation of “on-demand time,” and when they reach that daily limit, they’ll then be restricted to a limited number of skips per hour. Spotify did not share what that time limit is, but noted that Premium users will not have these restrictions.\n--------------------\nIn recent months, Spotify’s ad business has been struggling, with CEO Daniel Ek telling investors the company has been “moving too slowly” on this front. The streamer wants ad revenue to make up 20% of its overall revenue, but has grown it only to 11% as of June. By adding new free features, Spotify could boost engagement among its free user base, who would then be exposed to more ads.\n--------------------\nSpotify says that other features, like its support for lossless, AI Playlists, and Mix, will remain Premium-only offerings, while others like the newly launched Messages and personalized playlist daylist, available to global users, will span both the free and paid experiences, as they had before.\n--------------------\nThe company’s free users today make up the bulk of its user base. Out of Spotify’s 696 million monthly active users in the most recent quarter, 433 million were free, ad-supported customers. In addition, there were 276 million Premium (paying) subscribers in the quarter.\n--------------------\nUpdated after publication with more information about the restrictions impacting free users.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Vibe coding has turned senior devs into ‘AI babysitters,’ but they say it’s worth it",
    "url": "https://techcrunch.com/2025/09/14/vibe-coding-has-turned-senior-devs-into-ai-babysitters-but-they-say-its-worth-it/",
    "content": "Carla Rover once spent 30 minutes sobbing after having to restart a project she vibe coded.\n--------------------\nRover has been in the industry for 15 years, mainly working as a web developer. She’s now building a startup, alongside her son, that creates custom machine learning models for marketplaces.\n--------------------\nShe called vibe coding a beautiful, endless cocktail napkin on which one can perpetually sketch ideas. But dealing with AI-generated code that one hopes to use in production can be “worse than babysitting,” she said, as these AI models can mess up work in ways that are hard to predict.\n--------------------\nShe had turned to AI coding in a need for speed with her startup, as is the promise of AI tools.\n--------------------\n“Because I needed to be quick and impressive, I took a shortcut and did not scan those files after the automated review,” she said. “When I did do it manually, I found so much wrong. When I used a third-party tool, I found more. And I learned my lesson.”\n--------------------\nShe and her son wound up restarting their whole project — hence the tears. “I handed it off like the copilot was an employee,” she said. “It isn’t.”\n--------------------\nRover is like many experienced programmers turning to AI for coding help. But such programmers are also finding themselves acting like AI babysitters — rewriting and fact-checking the code the AI spits out.\n--------------------\nA recent report by content delivery platform company Fastly found that at least 95% of the nearly 800 developers it surveyed said they spend extra time fixing AI-generated code, with the load of such verification falling most heavily on the shoulders of senior developers.\n--------------------\nThese experienced coders have discovered issues with AI-generated code ranging from hallucinating package names to deleting important information and security risks. Left unchecked, AI code can leave a product far more buggy than what humans would produce.\n--------------------\nWorking with AI-generated code has become such a problem that it’s given rise to a new corporate coding job known as “vibe code cleanup specialist.”\n--------------------\nTechCrunch spoke to experienced coders about their time using AI-generated code about what they see as the future of vibe coding. Thoughts varied, but one thing remained certain: The technology still has a long way to go.\n--------------------\n“Using a coding co-pilot is kind of like giving a coffee pot to a smart six-year-old and saying, ‘Please take this into the dining room and pour coffee for the family,’” Rover said.\n--------------------\nCan they do it? Possibly. Could they fail? Definitely. And most likely, if they do fail, they aren’t going to tell you. “It doesn’t make the kid less clever,” she continued. “It just means you can’t delegate [a task] like that completely.”\n--------------------\nFeridoon Malekzadeh also compared vibe coding to a child.\n--------------------\nHe’s worked in the industry for more than 20 years, holding various roles in product development, software, and design. He’s building his own startup and heavily using vibe-coding platform Lovable, he said. For fun, he also vibe codes apps like one that generates Gen Alpha slang for Boomers.\n--------------------\nHe likes that he’s able to work alone on projects, saving time and money, but agrees that vibe coding is not like hiring an intern or a junior coder. Instead, vibe coding is akin to “hiring your stubborn, insolent teenager to help you do something,” he told TechCrunch.\n--------------------\n“You have to ask them 15 times to do something,” he said. “In the end, they do some of what you asked, some stuff you didn’t ask for, and they break a bunch of things along the way.”\n--------------------\nMalekzadeh estimates he spends around 50% of his time writing requirements, 10% to 20% of his time on vibe coding, and 30% to 40% of his time on vibe fixing — remedying the bugs and “unnecessary script” created by AI-written code.\n--------------------\nHe also doesn’t think vibe coding is the best at systems thinking — the process of seeing how a complex problem could impact an overall result. AI-generated code, he said, tries to solve more surface-level problems.\n--------------------\n“If you’re creating a feature that should be broadly available in your product, a good engineer would create that once and make it available everywhere that it’s needed,” Malekzadeh said. “Vibe coding will create something five different times, five different ways, if it’s needed in five different places. It leads to a lot of confusion, not only for the user, but for the model.”\n--------------------\nMeanwhile, Rover finds that AI “runs into a wall” when data conflicts with what it was hard-coded to do. “It can offer misleading advice, leave out key elements that are vital, or insert itself into a thought pathway you’re developing,” she said.\n--------------------\nShe also found that rather than admit to making errors, it will manufacture results.\n--------------------\nShe shared another example with TechCrunch, where she questioned the results an AI model initially gave her. The model started to give a detailed explanation pretending it used the data she uploaded. Only when she called it out did the AI model confess.\n--------------------\n“It freaked me out because it sounded like a toxic co-worker,” she said.\n--------------------\nOn top of this, there are the security concerns.\n--------------------\nAustin Spires is the senior director of developer enablement at Fastly and has been coding since the early 2000s.\n--------------------\nHe’s found through his own experience — along with chatting with customers — that vibe code likes to build what is quick rather than what is “right.” This may introduce vulnerabilities to the code of the kind that very new programmers tend to make, he said.\n--------------------\n“What often happens is the engineer needs to review the code, correct the agent, and tell the agent that they made a mistake,” Spires told TechCrunch. “This pattern is why we’ve seen the trope of ‘you’re absolutely right’ appear over social media.”\n--------------------\nHe’s referring to how AI models, like Anthropic Claude, tend to respond “you’re absolutely right” when called out on their mistakes.\n--------------------\nMike Arrowsmith, the chief trust officer at the IT management software company NinjaOne, has been in software engineering and security for around 20 years. He said that vibe coding is creating a new generation of IT and security blind spots to which young startups in particular are susceptible.\n--------------------\n“Vibe coding often bypasses the rigorous review processes that are foundational to traditional coding and crucial to catching vulnerabilities,” he told TechCrunch.\n--------------------\nNinjaOne, he said, counters this by encouraging “safe vibe coding,” where approved AI tools have access controls, along with mandatory peer review and, of course, security scanning.\n--------------------\nWhile nearly everyone we spoke to agrees that AI-generated code and vibe-coding platforms are useful in many situations — like mocking up ideas — they all agree that human review is essential before building a business on it.\n--------------------\n“That cocktail napkin is not a business model,” Rover said. “You have to balance the ease with insight.”\n--------------------\nBut for all the lamenting on its errors, vibe coding has changed the present and the future of the job.\n--------------------\nRover said vibe coding helped her tremendously in crafting a better user interface. Malekzadeh simply said that, despite the time he spends fixing code, he still gets more done with AI coders than without them.\n--------------------\n“‘Every technology carries its own negativity, which is invented at the same time as technical progress,” Malekzadeh said, quoting the French theorist Paul Virilio, who spoke about inventing the shipwreck along with the ship.\n--------------------\nThe Fastly survey found that senior developers were twice as likely to put AI-generated code into production compared to junior developers, saying that the technology helped them work faster.\n--------------------\nVibe coding is also part of Spires’ coding routine. He uses AI coding agents on several platforms for both front-end and back-end personal projects. He called the technology a mixed experience but said it’s good in helping with prototyping, building out boilerplate, or scaffolding out a test; it removes menial tasks so that engineers can focus on building, shipping, and scaling products.\n--------------------\nIt seems the extra hours spent combing through the vibe weeds will simply become a tolerated tax on using the innovation.\n--------------------\nElvis Kimara, a young engineer, is learning that now. He just graduated with a master’s in AI and is building an AI-powered marketplace.\n--------------------\nLike many coders, he said vibe coding has made his job harder and has often found vibe coding a joyless experience.\n--------------------\n“There’s no more dopamine from solving a problem by myself. The AI just figures it out,” he said. At one of his last jobs, he said senior developers didn’t look to help young coders as much — some not understanding new vibe-coding models, while others delegated mentorship tasks to said AI models.\n--------------------\nBut, he said, “the pros far outweigh the cons,” and he’s prepared to pay the innovation tax.\n--------------------\n“We won’t just be writing code; we’ll be guiding AI systems, taking accountability when things break, and acting more like consultants to machines,” Kimara said of the new normal for which he’s preparing.\n--------------------\n“Even as I grow into a senior role, I’ll keep using it,” he continued. “It’s been a real accelerator for me. I make sure I review every line of AI-generated code so I learn even faster from it.”\n--------------------\nThis post was updated to reflect the proper title of Mike Arrowsmith.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "‘Selling coffee beans to Starbucks’ — how the AI boom could leave AI’s biggest companies behind",
    "url": "https://techcrunch.com/2025/09/14/selling-coffee-beans-to-starbucks-how-the-ai-boom-could-leave-ais-biggest-companies-behind/",
    "content": "How much do foundation models matter?\n--------------------\nIt might seem like a silly question, but it’s come up a lot in my conversations with AI startups, which are increasingly comfortable with businesses that used to be dismissed as “GPT wrappers,” or companies that build interfaces on top of existing AI models like ChatGPT.\n--------------------\nThese days, startup teams are focused on customizing AI models for specific tasks and interface work, and see the foundation model as a commodity that can be swapped in and out as necessary. That approach was on display especially at last week’s BoxWorks conference, which seemed devoted entirely to the user-facing software built on top of AI models.\n--------------------\nPart of what is driving this is that the scaling benefits of pre-training — that initial process of teaching AI models using massive datasets, which is the sole domain of foundation models — has slowed down. That doesn’t mean AI has stopped making progress, but the early benefits of hyperscaled foundational models have hit diminishing returns, and attention has turned to post-training and reinforcement learning as sources of future progress.\n--------------------\nIf you want to make a better AI coding tool, you’re better off working on fine-tuning and interface design rather than spending another few billion dollars’ worth in server time on pre-training. As the success of Anthropic’s Claude Code shows, foundation model companies are quite good at these other fields too — but it’s not as durable an advantage as it used to be.\n--------------------\nIn short, the competitive landscape of AI is changing in ways that undermine the advantages of the biggest AI labs. Instead of a race for an all-powerful AGI that could match or exceed human abilities across all cognitive tasks, the immediate future looks like a flurry of discrete businesses: software development, enterprise data management, image generation, and so on.\n--------------------\nAside from a first-mover advantage, it’s not clear that building a foundation model gives you any advantage in those businesses. Worse, the abundance of open source alternatives means that foundation models may not have any price leverage if they lose the competition at the application layer. This would turn companies like OpenAI and Anthropic into back-end suppliers in a low-margin commodity business — as one founder put it to me, “like selling coffee beans to Starbucks.”\n--------------------\nIt’s hard to overstate what a dramatic shift this would be for the business of AI. Throughout the contemporary boom, the success of AI has been inextricable from the success of the companies building foundation models — specifically, OpenAI, Anthropic, and Google. Being bullish on AI meant believing that AI’s transformative impact would make these into generationally important companies. We could argue about which company would come out on top, but it was clear that some foundation model company was going to end up with the keys to the kingdom.\n--------------------\nAt the time, there were lots of reasons to think this was true. For years, foundation model development was the only AI business there was — and the fast pace of progress made their lead seem insurmountable. And Silicon Valley has always had a deep-rooted love of platform advantage. The assumption was that, however AI models ended up making money, the lion’s share of the benefit would flow back to the foundation model companies, which had done the work that was hardest to replicate.\n--------------------\nThe past year has made that story more complicated. There are lots of successful third-party AI services, but they tend to use foundation models interchangeably. For startups, it no longer matters whether their product sits on top of GPT-5, Claude, or Gemini, and they expect to be able to switch models in mid-release without end users noticing the difference. Foundation models continue to make real progress, but it no longer seems plausible for any one company to maintain a large enough advantage to dominate the industry.\n--------------------\nWe already have plenty of indication that there is not much of a first-mover advantage. As venture capitalist Martin Casado of a16z pointed out on a recent podcast, OpenAI was the first lab to put out a coding model, as well as generative models for image and video — only to lose all three categories to competitors. “As far as we can tell, there is no inherent moat in the technology stack for AI,” Casado concluded.\n--------------------\nOf course, we shouldn’t count foundation model companies out just yet. There are still lots of durable advantages on their side, including brand recognition, infrastructure, and unthinkably vast cash reserves. OpenAI’s consumer business may prove harder to replicate than its coding business, and other advantages may emerge as the sector matures. Given the fast pace of AI development, the current interest in post-training could easily reverse course in the next six months. Most uncertain of all, the race toward general intelligence could pay off with new breakthroughs in pharmaceuticals or materials science, radically shifting our ideas about what makes AI models valuable.\n--------------------\nBut in the meantime, the strategy of building ever-bigger foundation models looks a lot less appealing than it did last year — and Meta’s billion-dollar spending spree is starting to look awfully risky.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Why the Oracle-OpenAI deal caught Wall Street by surprise",
    "url": "https://techcrunch.com/2025/09/12/why-the-oracle-openai-deal-caught-wall-street-by-surprise/",
    "content": "This week, OpenAI and Oracle shocked the markets with a surprise $300 billion, five-year agreement, part of a surge of new business that sent the cloud provider’s stock skyrocketing. But maybe the markets shouldn’t have been taken by surprise. The deal is a reminder that, despite Oracle’s legacy status, the company still plays a major role in AI infrastructure.\n--------------------\nOn the OpenAI side, the agreement was more revealing than the lack of details suggest. For one, the startup’s willingness to pay so much for compute provides a measurement of the startup’s appetite — even if it’s unclear where the electricity to power said compute is coming from or how it will pay for it.\n--------------------\nChirag Dekate, a vice president at research firm Gartner, told TechCrunch it’s clear why both sides were interested in this deal. It makes sense for OpenAI to work with several infrastructure providers, he noted. It also diversifies the company’s infrastructure — spreading out risk among several cloud providers — and gives OpenAI a scaling advantage compared to competitors.\n--------------------\n“OpenAI seems to be putting together one of the most comprehensive global AI supercomputing foundations for extreme scale, inference scaling where appropriate,” Dekate said. “This is quite unique. This is probably exemplary of what a model ecosystem should look like.”\n--------------------\nSome industry watchers expressed surprise that Oracle was involved, citing the company’s diminished role in the AI boom compared to cloud rivals like Google, Microsoft Azure, and AWS. But Dekate argues that observers shouldn’t be so surprised: Oracle has worked with hyperscalers before and provides the infrastructure for TikTok’s sizable U.S. business.\n--------------------\n“Over the decades, they actually built core infrastructure capabilities that enabled them to deliver extreme scale and performance as a core part of their cloud infrastructure,” Dekate said.\n--------------------\nBut even as the stock market celebrates the deal, key details are missing and questions around power and payment remain.\n--------------------\nOpenAI has made a string of infrastructure investment announcements over the past year, each one with an eye-popping price tag. OpenAI has committed to spend around $60 billion a year for compute from Oracle and $10 billion to develop custom AI chips with Broadcom.\n--------------------\nMeanwhile, OpenAI said in June it hit $10 billion in annual recurring revenue, up from around $5.5 billion last year. That figure includes revenue from the company’s consumer products, ChatGPT business products, and its API. And while its CEO Sam Altman has painted a rosy picture of its future prospects in terms of subscribers, products, and revenue, the company is burning through billions of dollars in cash each year.\n--------------------\nPower is another question, or more specifically where the companies plan to source the energy needed to run this level of compute.\n--------------------\nIndustry observers have been predicting a near-term boost for natural gas, though solar and batteries are arguably better positioned to deliver power sooner and at lower cost in many markets. Tech companies are also betting big on nuclear.\n--------------------\nDespite market moving headlines, the energy impact of OpenAI’s anticipated growth isn’t entirely unexpected. Data centers are anticipated to consume 14% of all electricity in the U.S. by 2040, according to a report the Rhodium Group published yesterday.\n--------------------\nCompute has always been a constraint for AI companies, so much so that investors have bought thousands of Nvidia chips to ensure their startups have access to the power they need. Andreessen Horowitz has reportedly purchased over 20,000 GPUs, while Nat Friedman and Daniel Gross rented access to a 4,000 GPU cluster (though maybe Meta owns that now).\n--------------------\nBut compute is worthless without power. To ensure their data centers remain juiced, large tech companies have been snapping up solar farms, buying nuclear power plants, and inking deals with geothermal startups.\n--------------------\nSo far, OpenAI has been relatively quiet on that front. CEO Sam Altman has placed several prominent bets in the energy sector, including Oklo, Helion, and Exowatt, but the company itself hasn’t thrown money into the space like Google, Meta, or Amazon.\n--------------------\nWith a 4.5 gigawatt compute deal, that may soon change.\n--------------------\nThe company may play an indirect role, paying Oracle to handle the physical infrastructure — something it has extensive experience with — just as Altman invested in startups aligned with OpenAI’s future power needs. That will leave the company “asset light,” something that will undoubtedly please its investors and help keep its valuation in line with other software-centric AI startups and not with legacy tech firms, which are burdened with pricey infrastructure.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Elon Musk’s Boring Company suspends work on Vegas airport tunnel after ‘crushing injury’",
    "url": "https://techcrunch.com/2025/09/11/elon-musks-boring-company-suspends-work-on-vegas-airport-tunnel-after-crushing-injury/",
    "content": "Posted:\n--------------------\nElon Musk’s Boring Company has reportedly stopped work on a tunnel it’s been digging to the Las Vegas airport after a worker sustained a “crushing injury” late Wednesday night, according to Fortune.  Nevada’s Occupational Safety and Health Administration (OSHA) has opened an investigation.\n--------------------\nThe Clark County Fire Department received a call at 10:12 p.m. local time on Wednesday and dispatched an 18-person rescue crew to the site. It’s unclear exactly what happened to the worker, but the individual was lifted out of the job site by the local fire department, which used an on-site crane. The fire department told Fortune the worker is “reported to be stable.”\n--------------------\nThe Boring Company has spent the last few years digging tunnels that connect the Las Vegas Convention Center with a small number of nearby hotel casinos. The underground transportation system has provided over 3 million rides across the 3.5 miles of tunnels currently in service. The company has much larger ambitions of connecting nearly all of Las Vegas by underground tunnels, including the airport.\n--------------------\nBut dozens of workers have been injured during construction of these tunnels, and even the company’s former safety manager for the Las Vegas project has publicly voiced concerns about the dangers faced at these sites.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "Atlassian acquires DX, a developer productivity platform, for $1B",
    "url": "https://techcrunch.com/2025/09/18/atlassian-acquires-dx-a-developer-productivity-platform-for-1b/",
    "content": "Productivity software giant Atlassian is making its largest acquisition yet to add a developer productivity tool to its product suite.\n--------------------\nAtlassian announced Thursday it has agreed to acquire the developer productivity insight platform DX for $1 billion in cash and restricted stock. Enterprises use DX to analyze how productive their engineering teams are and identify bottlenecks slowing them down.\n--------------------\nDX was launched five years ago by Abi Noda and Greyson Junggren. Noda told TechCrunch in 2022 that he founded the company to find a better way to understand what hampered engineering teams. At the time, he felt the metrics he was using as a product manager at GitHub weren’t giving him the full picture, and he wanted to build something better that didn’t make developers feel like they were being surveilled.\n--------------------\n“The assumptions we had about what we needed to help ship products faster were quite different than what the teams and developers were saying was getting in their way,” Noda told TechCrunch at the time. “Even teams didn’t always have awareness about their own issues and leadership.”\n--------------------\nDX came out of stealth in 2022 and has since tripled its customer base every year. The company now works with more than 350 enterprise customers, including ADP, Adyen and GitHub, among others, while raising less than $5 million in venture funding.\n--------------------\nAtlassian co-founder and CEO Mike Cannon-Brookes told TechCrunch that after trying to build an in-house developer productivity insight tool for three years, his Sydney, Australia-based company realized it made sense to look for an external, existing option.\n--------------------\nDX was a natural choice, Cannon-Brookes said, considering 90% of DX’s customers were already used Atlassian’s project management and collaboration tools as well.\n--------------------\n“DX has done an amazing job [of] understanding the qualitative and quantitative aspects of developer productivity and turning that into actions that can improve those companies and give them insights and comparisons to others in their industry, others at their size, etc.,” Cannon-Brookes said.\n--------------------\nHe added that the timing was right due to the rise of AI tools and companies looking for ways to measure how they are being used.\n--------------------\n“You suddenly have these budgets that are going up. Is that a good thing?” Cannon-Brookes said. “Is that not a good thing?  Am I spending the money in the right ways? It’s really, really important and critical.”\n--------------------\nHe added that there was a great cultural fit, too. Cannon-Brookes said he’s always felt an affinity for Utah-based entrepreneurs — DX is based in Salt Lake City — and he liked that both companies were able to scale without taking on much outside funding.\n--------------------\nThe feeling was mutual.\n--------------------\nNoda told TechCrunch this week that he thinks DX and Atlassian are better together than apart and that many of Atlassian’s tools are complementary to the data and information that DX’s platform gathers.\n--------------------\n“We are able to provide customers with that full flywheel to get the data and understand where we are unhealthy,” Noda said. “They can plug in Atlassian’s tools and solutions to go address those bottlenecks. An end-to-end flywheel that is ultimately what customers want.”\n--------------------\nDX’s platform will be integrated into the broader Atlassian product suite.\n--------------------\nThis is Atlassian’s second acquisition this month. The company announced it was buying AI-browser developer The Browser Company in early September.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n",
    "has_been_pretreat": false
  },
  {
    "title": "OpenAI’s research on AI models deliberately lying is wild",
    "url": "https://techcrunch.com/2025/09/18/openais-research-on-ai-models-deliberately-lying-is-wild/",
    "content": "Every now and then, researchers at the biggest tech companies drop a bombshell. There was the time Google said its latest quantum chip indicated multiple universes exist. Or when Anthropic gave its AI agent Claudius a snack vending machine to run and it went amok, calling security on people and insisting it was human.\n--------------------\nThis week, it was OpenAI’s turn to raise our collective eyebrows.\n--------------------\nOpenAI released on Monday some research that explained how it’s stopping AI models from “scheming.” It’s a  practice in which an “AI behaves one way on the surface while hiding its true goals,” OpenAI defined in its tweet about the research.\n--------------------\nIn the paper, conducted with Apollo Research, researchers went a bit further, likening AI scheming to a human stock broker breaking the law to make as much money as possible. The researchers, however, argued that most AI “scheming” wasn’t that harmful. “The most common failures involve simple forms of deception — for instance, pretending to have completed a task without actually doing so,” they wrote.\n--------------------\nThe paper was mostly published to show that “deliberative alignment⁠” — the anti-scheming technique they were testing — worked well.\n--------------------\nBut it also explained that AI developers haven’t figured out a way to train their models not to scheme. That’s because such training could actually teach the model how to scheme even better to avoid being detected.\n--------------------\n“A major failure mode of attempting to ‘train out’ scheming is simply teaching the model to scheme more carefully and covertly,” the researchers wrote.\n--------------------\nPerhaps the most astonishing part is that, if a model understands that it’s being tested, it can pretend it’s not scheming just to pass the test, even if it is still scheming. “Models often become more aware that they are being evaluated. This situational awareness can itself reduce scheming, independent of genuine alignment,” the researchers wrote.\n--------------------\nIt’s not news that AI models will lie. By now most of us have experienced AI hallucinations, or the model confidently giving an answer to a prompt that simply isn’t true. But hallucinations are basically presenting guesswork with confidence, as OpenAI research released earlier this month documented.\n--------------------\nScheming is something else. It’s deliberate.\n--------------------\nEven this revelation — that a model will deliberately mislead humans — isn’t new. Apollo Research first published a paper in December documenting how five models schemed when they were given instructions to achieve a goal “at all costs.”\n--------------------\nThe news here is actually good news: The researchers saw significant reductions in scheming by using “deliberative alignment⁠.” That technique involves teaching the model an “anti-scheming specification” and then making the model go review it before acting. It’s a bit like making little kids repeat the rules before allowing them to play.\n--------------------\nOpenAI researchers insist that the lying they’ve caught with their own models, or even with ChatGPT, isn’t that serious. As OpenAI’s co-founder Wojciech Zaremba told TechCrunch’s Maxwell Zeff about this research: “This work has been done in the simulated environments, and we think it represents future use cases. However, today, we haven’t seen this kind of consequential scheming in our production traffic. Nonetheless, it is well known that there are forms of deception in ChatGPT. You might ask it to implement some website, and it might tell you, ‘Yes, I did a great job.’ And that’s just the lie. There are some petty forms of deception that we still need to address.”\n--------------------\nThe fact that AI models from multiple players intentionally deceive humans is, perhaps, understandable. They were built by humans, to mimic humans, and (synthetic data aside) for the most part trained on data produced by humans.\n--------------------\nIt’s also bonkers.\n--------------------\nWhile we’ve all experienced the frustration of poorly performing technology (thinking of you, home printers of yesteryear), when was the last time your not-AI software deliberately lied to you? Has your inbox ever fabricated emails on its own? Has your CMS logged new prospects that didn’t exist to pad its numbers? Has your fintech app made up its own bank transactions?\n--------------------\nIt’s worth pondering this as the corporate world barrels toward an AI future where companies believe agents can be treated like independent employees. The researchers of this paper have the same warning.\n--------------------\n“As AIs are assigned more complex tasks with real-world consequences and begin pursuing more ambiguous, long-term goals, we expect that the potential for harmful scheming will grow — so our safeguards and our ability to rigorously test must grow correspondingly,” they wrote.\n--------------------\n\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Huawei announces new AI infrastructure as Nvidia gets locked out of China",
    "url": "https://techcrunch.com/2025/09/18/huawei-announces-new-ai-infrastructure-as-nvidia-gets-locked-out-of-china/",
    "content": "Posted:\n--------------------\nTech giant Huawei unveiled new AI infrastructure meant to help boost compute power and allow the company to better compete with rival chipmaker Nvidia.\n--------------------\nAt a keynote at its Huawei Connect conference on Thursday, Shenzhen, China-based Huawei announced new SuperPoD Interconnect technology that can link together up to 15,000 graphics cards, including Huawei’s Ascend AI chips, to increase compute power.\n--------------------\nThis tech seems to be a competitor for Nvidia’s NVLink infrastructure, which facilitates high-speed communication between AI chips.\n--------------------\nTechnology like this is critical for Huawei to better compete with semiconductors like Nvidia’s. While Huawei’s AI chips are less powerful than Nvidia’s, being able to cluster them together will give its users access to more compute power, which is needed for training and scaling AI systems.\n--------------------\nThis news also comes just a day after China banned domestic tech companies from buying Nvidia’s hardware, including Nvidia’s RTX Pro 600D servers specifically designed for the market in China.\n--------------------\nTechCrunch reached out to Huawei for more information.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "How AI startups are fueling Google’s booming cloud business",
    "url": "https://techcrunch.com/2025/09/18/how-ai-startups-are-fueling-googles-booming-cloud-business/",
    "content": "Google Cloud announced Thursday it has added fast-rising AI coding startups Lovable and Windsurf to its roster of customers. Both companies have chosen Google Cloud as their primary cloud computing provider, the latest sign of Google’s rising prominence against larger rivals AWS and Microsoft Azure.\n--------------------\nThe deals also highlight Google’s efforts to make its cloud business more central to the company’s future.\n--------------------\nToday, Google Cloud is overshadowed by larger competitors like AWS and Microsoft, as well as Google’s much larger advertising business. But it is seeing upward momentum.\n--------------------\nGoogle Cloud is one of the company’s fastest-growing business lines. On its last earnings call, Google said its cloud division hit an annual run rate of $50 billion, and cloud chief Thomas Kurian said this week the unit lined up $58 billion in new revenue over the next two years. Google generated $43.2 billion in cloud services in 2024 and $33.1 billion in 2023.\n--------------------\nWinning contracts with leading AI startups seems to be a large driver of Google Cloud’s growth. The division says it now works with nine out of the 10 leading AI labs, including Safe Superintelligence and OpenAI, and 60% of the world’s generative AI startups. In the last year, the company says it’s seen a 20% increase in the number of new AI startups choosing Google Cloud.\n--------------------\nWhile Lovable and Windsurf, which was recently acquired by Cognition, spend relatively little compared to leading AI labs or large enterprises, the bet is that they will become larger businesses in the future, and well worth the investment.\n--------------------\nGoogle says the two vibe-coding startups use Gemini 2.5 Pro to power their products, which are also run on Google Cloud infrastructure. Google says Windsurf is also using Gemini models in integrations with Cognition’s AI agent, Devin.\n--------------------\nThe significant cloud costs of training, fine-tuning, and running AI models has presented a major challenge for AI model developers, including Google DeepMind with its Gemini models. But it’s been a boon for cloud businesses. The global cloud market is expected to exceed $400 billion in 2025 and grow at a rate of 20% over the next five years, according to the market intelligence and analytics firm Synergy Research.\n--------------------\nThe company on Thursday hosted its first Google AI Builder’s Forum, in which it brought together hundreds of AI startup founders and announced more than 40 new AI startups building on Google Cloud. In addition to Lovable and Windsurf, Sequoia-backed Factory AI and Andreessen Horowitz-backed Krea AI were among the customers named.\n--------------------\nPart of the reason so many AI startups work with Google Cloud are the generous deals it offers. Many of the AI startups Google works with started on its Google for Startups Cloud Program, in which it offers $350,000 in cloud credits. Google Cloud also offers a dedicated cluster of Nvidia GPUs for startups in the Y Combinator accelerator program.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Tim Cook, Sam Altman, and more attend Trump’s UK state banquet",
    "url": "https://techcrunch.com/2025/09/18/tim-cook-sam-altman-and-more-attend-trumps-uk-state-banquet/",
    "content": "Top tech names were on the guest list for the banquet thrown for President Trump during his second state visit to the U.K. on Wednesday.\n--------------------\nThe banquet seating chart included Nvidia CEO Jensen Huang; Apple CEO Tim Cook; venture capitalist and White House AI and crypto czar David Sacks; Alphabet and Google president Ruth Porat; Microsoft CEO Satya Nadella; Salesforce CEO Marc Benioff; and OpenAI’s Sam Altman, according to the New York Times.\n--------------------\nOn Thursday, the U.S. and U.K. signed a partnership called the Tech Prosperity Deal to focus on developing nuclear, AI, and quantum technologies. Google, Microsoft, Nvidia, and OpenAI also made announcements earlier this week to build data centers in the U.K., while CoreWeave and Salesforce announced a multibillion-pound investment in the country. Overall, American tech firms committed a total of £31 billion ($42 billion) to boost AI infrastructure in the U.K.\n--------------------\nThis state banquet guest list seems to have featured more tech and business names than the Hollywood types that often attend such affairs.\n--------------------\nThis change reveals the shifting economic needs of the U.K. and U.S. in the age of AI, as well as the rising prominence of technology and its leaders in Trump’s second administration. Just this past year, numerous Big Tech companies like OpenAI, Google, and Apple have pledged to work with the government, from providing AI assistant tools to government services to building digital health ecosystems for the U.S. health industry.\n--------------------\nThe president has also taken a sharper focus on tech — criticizing Tim Cook for Apple’s outsourced supply chain, signing an “anti-woke” AI order, and instructing the attorney general to investigate private companies receiving federal funds that have DEI programs deemed “illegal.”\n--------------------\nMark Zuckerberg, Jeff Bezos, and other tech leaders attended the president’s inauguration this year. And, in early September, President Trump threw a tech dinner with 33 top names in Silicon Valley, including Altman, Cook, and Zuckerberg. Musk, a former senior adviser to the president, once known as “First Buddy,” was not present at either dinner.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Notion launches agents for data analysis and task automation",
    "url": "https://techcrunch.com/2025/09/18/notion-launches-agents-for-data-analysis-and-task-automation/",
    "content": "At the “Make with Notion” event on Thursday, the company announced the launch of its first AI agent. The agent will draw on all of a user’s notion pages and database as context, automatically generating notes and analysis for meetings, competitor evaluation reports, and feedback landing pages.\n--------------------\nThe productivity platform said that the agent can create pages and databases, or update them with new data, properties, or views. Users can also trigger Notion agents from outside platforms that are linked to the service. For instance, you can ask the Notion agent to create a bug-tracking dashboard from various sources, including Slack, email, and Google Drive.\n--------------------\nThe newly announced Agent builds on Notion AI, a pre-existing feature that could search or summarize content. But the new agent is able to tackle more complex multistep tasks, using the powers of agentic AI. The company said that the current version of the agent can perform a task that runs up to 20 minutes across hundreds of pages.\n--------------------\nUsers can set up a “profile” page for the agent to instruct it to follow directions on referencing sources, style of output, and where to update tasks and final results. You’ll also be able to ask the agent to “remember” key points as people use them. Those memories will be stored on the profile page, and users can edit them there.\n--------------------\nIn demo videos, the company gave examples of agents that could provide feedback for landing pages and update them, create a restaurant tracker, create an analysis from meeting notes, and prepare a competition analysis report.\n--------------------\nAt the moment, you have to trigger these actions manually. But Notion said that the ability to create customized agents that work on a schedule or triggers is coming soon. The company will also release a template library for agents so you can pick ready-made prompts that might suit your task.\n--------------------\nOver the last two years, Notion has released a calendar app, a Gmail client, a meeting note-taker, and an enterprise search to get information from different sources. These are features that gave the company enough contextual building blocks to create automations. Other enterprise knowledge and productivity platforms, including Salesforce, Fireflies, and Read AI have launched their own agents to extract and update information.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Google now lets you share your custom Gemini AI assistants known as Gems",
    "url": "https://techcrunch.com/2025/09/18/google-now-lets-you-share-your-custom-gemini-ai-assistants-known-as-gems/",
    "content": "Posted:\n--------------------\nGoogle is making it possible to now share your Gemini Gems — custom AI assistants and experts designed for specific tasks — the company announced on Thursday. The feature launched last year, initially as part of the Gemini Advanced paid subscription, allowing users to write instructions to create an AI chatbot for different scenarios. For instance, Google launched with premade Gems like a learning coach, a brainstorming assistant, a career guide, a writing editor, and a coding partner.\n--------------------\nNow Google says you’ll be able to share your Gems with friends, family, or co-workers as easily as you can share a file from Google Drive.\n--------------------\nThis would make Gems more accessible to more people, as not everyone uses the advanced customization feature. It could also help prevent people from building the same Gems as others. For instance, if multiple co-workers were using a similar type of custom Gemini assistant, they could just share the same resource instead of each making their own version that could have slight inconsistencies between them.\n--------------------\nGoogle suggests Gem sharing could also be useful for people working on family vacation plans and guides, meal planners, or collaborative writing projects.\n--------------------\nTo share a Gem, you’ll open the Gem manager on the web app and click the “Share” icon next to any Gem you’ve created. Also similar to Google Drive, you can control who can view and use your Gems and who’s allowed to edit them.\n--------------------\nAfter first rolling out to Gemini Advanced, Gemini Business, and Gemini Enterprise subscribers in over 150 countries, Google announced in March that Gems were now available to everyone and could support file uploads.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Mark Zuckerberg has begun his quest to kill the smartphone",
    "url": "https://techcrunch.com/2025/09/18/mark-zuckerberg-has-begun-his-quest-to-kill-the-smartphone/",
    "content": "If you can’t resist the urge to check your phone over and over, even if you’re out with friends, Meta has a solution: check your glasses instead.\n--------------------\n“The promise of glasses is to preserve this sense of presence that you have with other people,” said CEO Mark Zuckerberg at the Meta Connect 2025 keynote. “I think that we’ve lost it a little bit with phones, and we have the opportunity to get it back with glasses.”\n--------------------\nIn reality, Meta wants its own hardware to eat into the marketshare of Apple and Google so that it doesn’t have to keep siphoning profits to them via app stores. But nevertheless, this is the angle Meta is taking to sell its most sophisticated smart glasses yet, the Meta Ray-Ban Display, which the company hopes could one day eclipse the market share of smartphones.\n--------------------\nMeta’s Reality Labs division burns cash at an alarming rate, which has concerned investors over the years. But Wednesday’s event finally showed us a glimpse of what the division’s $70 billion in losses since 2020 have gone toward.\n--------------------\nMeta has had its fair share of flops, like the entire promise of its social metaverse. (Remember when they announced that metaverse avatars would finally get legs?) But with the Meta Ray-Ban Display, Meta has created a remarkable piece of technology, unlike any other consumer-facing product on the market — we have yet to test it ourselves, so we can’t quite say just how groundbreaking this really is, but it looks promising.\n--------------------\nLike Meta’s existing smart glasses, which have sold millions of pairs, the new model has cameras, speakers, microphones, and an on-board AI assistant. The display on the glasses, which is offset so as not to obstruct one’s sightline, can display Meta apps like Instagram, WhatsApp, and Facebook, as well as directions and live translations.\n--------------------\nWhat most sets the Meta Ray-Ban Display apart is the Meta Neural Band, a wristband that uses surface electromyography (sEMG) to pick up on signals sent between your brain and your hand when performing a gesture.\n--------------------\nMeta’s keynote didn’t get into the specifics of how Zuckerberg was writing these texts, but according to Reality Labs’ research on sEMG, users can write out messages like this by holding their fingers together as if they were gripping a pen and “writing” out the text.\n--------------------\nWhile some live AI demos at the keynote failed — Zuckerberg blamed the Wi-Fi — we at least got to see the wristband in action, which is more novel. Zuckerberg quickly wrote out text messages, then sent them on his Ray-Bans.\n--------------------\n“I’m up to about 30 words a minute on this,” Zuckerberg said onstage at the company’s Menlo Park headquarters. “You can get pretty fast.”\n--------------------\nOn a touchscreen smartphone like an iPhone, research has estimated that people text at about 36 words per minute, making Zuckerberg’s claim impressive. Reality Labs’ research participants averaged closer to 21 words per minute.\n--------------------\nUnlike past Meta Ray-Bans, this technology allows people to actually use the glasses without speaking aloud, which isn’t always natural in public settings. While Apple Watch users can send texts without voice prompting, the process is so tedious and slow that it’s only useful as a last resort.\n--------------------\nOther gesture controls on the wristband seem more similar to technology that consumers have used before, like Nintendo Joy-Cons and Apple Watches. But if the voiceless texting interface is as good as it seems, then the wristband will likely be capable of more complex gestures than we’re used to.\n--------------------\nMeta has invested heavily in research on sEMG since 2021, even showing us a prototype of a heftier product called Orion. Like Apple and Google, Meta is preparing for a not-so-impossible future where these smart glasses could potentially eclipse the smartphone.\n--------------------\nBut as is the risk with any massive hardware investment, there’s no way to know if this will actually feel more natural to people in their day-to-day lives than pulling a sleek aluminum rectangle out of their pocket to tap out messages to their friends.\n--------------------\nThis might be Meta’s biggest bet — perhaps a bigger bet than its subpar metaverse. That’s why it’s so striking that Zuckerberg is unveiling this technology as not just a fascinating innovation, but something that he wants to portray as more prosocial than the smartphone. It’s a way for him to capitalize on our growing malaise with our ever-increasing screen time, even though he’s the one making the apps that demand our attention.\n--------------------\n“The technology needs to get out of the way,” Zuckerberg said.\n--------------------\nWill the smartphone become an obsolete relic like a Nokia with a T9 keyboard? That depends on whether there’s truth to Zuckerberg’s narrative that these glasses will help us feel more present. But Meta and its competitors are betting big on the cultural shift from smartphones to smart glasses, and the Ray-Ban Display will give consumers their first taste of this possible future.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Google brings Gemini in Chrome to US users, unveils agentic browsing capabilities, and more",
    "url": "https://techcrunch.com/2025/09/18/google-brings-gemini-in-chrome-to-us-users-unveils-agentic-browsing-capabilities-and-more/",
    "content": "Google announced Thursday that it’s rolling out Gemini in Chrome to all Mac and Windows desktop users in the U.S. after previously limiting the capability to Google AI Pro and Google AI Ultra subscribers. The tech giant also announced that it’s bringing agentic capabilities to Chrome in the future, adding its AI Mode search feature to the address bar, launching new Gemini features, using AI to combat AI-generated scams, rolling out automatic password resets, and more.\n--------------------\nU.S. users who have their language set to English can now ask Gemini to clarify complex information on any web page they’re reading using the Gemini icon in the top-right corner of their Chrome window. For example, you could open up a page that features a banana bread recipe and ask Gemini to make the recipe gluten free.\n--------------------\nGemini can now work across multiple tabs, allowing users to quickly compare and summarize information across multiple websites. For example, you could be planning your flight, hotel, and vacation in multiple tabs and work with Gemini to organize your trip. Or, you might be shopping for a new mattress and want to compare all of the different models you’re looking at in multiple tabs.\n--------------------\nGemini will soon be able to retrieve web pages you’ve previously visited, making it easier to return to past browsing sessions without sifting through your browser history. That means you will be able to ask something like “On which site did I see the walnut desk last week?” or “What was that blog I read on back-to-school shopping?”\n--------------------\nAdditionally, Google is launching a deeper integration between Gemini in Chrome and other Google apps, like Calendar, YouTube, and Maps. Google says this will allow users to do things like schedule meetings, see location details, and more without having to leave the page they’re on. For example, if you’re trying to find a specific spot in a YouTube video, you can ask Gemini to take you there.\n--------------------\nGoogle notes that the AI assistant will be able to complete tedious tasks, like booking a haircut or ordering weekly groceries. Gemini will navigate to the site, add things to your cart, and let you take the final action by checking out with your payment option.\n--------------------\nGoogle says the new agentic capabilities will be available in Chrome in the coming months. It’s worth noting that OpenAI launched Operator, an AI agent that performs tasks autonomously, earlier this year.\n--------------------\nGoogle is also bringing AI Mode, its advanced search feature, directly into the Chrome address bar. With AI Mode, users can ask complex questions with follow-ups to dig deeper into topics. For example, instead of searching for “best mattress,” you could type out “I’m a side sleeper with occasional lower back pain. Make me a table comparing the different mattress types” directly in the address bar. From there, you could ask follow-up questions and keep your search going with queries like, “How long do memory foam mattresses typically last?”\n--------------------\nThis update will be rolling out later this month in English in the U.S. and expanding to more countries and languages in the future.\n--------------------\nAlso coming to the address bar is the ability to ask questions about the page you’re on. Chrome can now suggest relevant questions based on the context of the page to kickstart your search in the address bar. Google says users will get a helpful AI Overview and the option to ask follow-up questions with AI Mode.\n--------------------\nThe company says Chrome will also soon be able to use its Gemini Nano model to detect and protect against scams, such as fake virus alerts and fraudulent giveaways. These scams often impersonate trusted brands and use generative AI to create convincing phishing attempts, Google notes.\n--------------------\nGoogle also announced that it’s using AI to help users fix compromised passwords with a single click on supported sites, like Coursera, Spotify, Duolingo, H&M, and more. If Chrome warns you that your password was exposed in a data breach, you can allow it to create and save a new one for you.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "ChatGPT: Everything you need to know about the AI-powered chatbot",
    "url": "https://techcrunch.com/2025/09/18/chatgpt-everything-to-know-about-the-ai-chatbot/",
    "content": "ChatGPT, OpenAI’s text-generating AI chatbot, has taken the world by storm since its launch in November 2022. What started as a tool to supercharge productivity through writing essays and code with short text prompts has evolved into a behemoth with 300 million weekly active users.\n--------------------\n2024 was a big year for OpenAI, from its partnership with Apple for its generative AI offering, Apple Intelligence, the release of GPT-4o with voice capabilities, and the highly-anticipated launch of its text-to-video model Sora.\n--------------------\nOpenAI also faced its share of internal drama, including the notable exits of high-level execs like co-founder and longtime chief scientist Ilya Sutskever and CTO Mira Murati. OpenAI has also been hit with lawsuits from Alden Global Capital-owned newspapers alleging copyright infringement, as well as an injunction from Elon Musk to halt OpenAI’s transition to a for-profit.\n--------------------\nIn 2025, OpenAI is battling the perception that it’s ceding ground in the AI race to Chinese rivals like DeepSeek. The company has been trying to shore up its relationship with Washington as it simultaneously pursues an ambitious data center project, and as it reportedly lays the groundwork for one of the largest funding rounds in history.\n--------------------\nBelow, you’ll find a timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year. If you have any other questions, check out our ChatGPT FAQ here.\n--------------------\nTo see a list of 2024 updates, go here.\n--------------------\n\n--------------------\nCEO Sam Altman announced new policies for under-18 users of ChatGPT, tightening safeguards around sensitive conversations. The company says it will block flirtatious exchanges with minors and add stronger protections around discussions of suicide, even escalating severe cases to parents or authorities. The move comes as OpenAI faces a wrongful death lawsuit tied to alleged chatbot interactions, underscoring rising concerns about the mental health risks of AI companions.\n--------------------\nOpenAI rolled out GPT-5-Codex, a new version of its AI coding agent that can spend anywhere from a few seconds to seven hours tackling a task, depending on complexity. The company says this dynamic approach helps the model outperform GPT-5 on key coding benchmarks, including bug fixes and large-scale refactoring. The update comes as OpenAI looks to keep Codex competitive in a fast-growing market that now includes rivals like Claude Code, Cursor, and GitHub Copilot.\n--------------------\nOpenAI is shaking up its Model Behavior team, the small but influential group that helps shape how its AI interacts with people The roughly 14-person team is being folded into the larger Post Training group, now reporting to lead researcher Max Schwarzer. Meanwhile, founding leader Joanne Jang is spinning up a new unit called OAI Labs, focused on prototyping fresh ways for people to collaborate with AI.\n--------------------\nOpenAI, facing a lawsuit from the parents of a 16-year-old who died by suicide, said in its blog that it has implemented new safeguards for ChatGPT, including stronger detection of mental health risks and parental control features. The AI company said the updates aim to provide tighter protections around suicide-related conversations and give parents more oversight of their children’s use.\n--------------------\nElon Musk’s AI startup, xAI, filed a federal lawsuit in Texas against Apple and OpenAI, alleging that the two companies colluded to lock up key markets and shut out rivals.\n--------------------\nOpenAI introduced its most affordable subscription plan, ChatGPT Go, in India, priced at 399 rupees per month (approximately $4.57). This move aims to expand OpenAI’s presence in its second-largest market, offering enhanced access to the latest GPT-5 model and additional features.\n--------------------\nSince its May 2023 launch, ChatGPT’s mobile app has amassed $2 billion in global consumer spending, dwarfing competitors like Claude, Copilot, and Grok by roughly 30 times, according to Appfigures. This year alone, the app has generated $1.35 billion, a 673% increase from the same period in 2024, averaging nearly $193 million per month, or 53 times more than its nearest rival, Grok.\n--------------------\nDespite unveiling GPT-5 as a “one-size-fits-all” AI, OpenAI is still offering several legacy AI options, including GPT-4o, GPT-4.1, and o3. Users can choose between new “Auto,” “Fast,” and “Thinking” modes for GPT-5, and paid subscribers regain access to legacy models like GPT-4o and GPT-4.1.\n--------------------\nOpenAI CEO Sam Altman told Reddit users that GPT-5’s “dumber” behavior at launch was due to a router issue and promised fixes, double rate limits for Plus users, and transparency on which model is answering, while also shrugging off the infamous “chart crime” from the live presentation.\n--------------------\nOpenAI released GPT-5, a next-gen AI that’s not just smarter but more useful — able to handle tasks like coding apps, managing calendars, and creating research briefs — while automatically figuring out the fastest or most thoughtful way to answer your questions.\n--------------------\nOpenAI is making a major push into federal government workflows, offering ChatGPT Enterprise to agencies for just $1 for the next year. The move comes after the U.S. General Services Administration (GSA) added OpenAI, Google, and Anthropic to its approved AI vendor list, allowing agencies to access these tools through preset contracts without negotiating pricing.\n--------------------\nOpenAI unveiled its first open source language models since GPT-2, introducing two new open-weight AI releases: gpt-oss-120b, a high-performance model capable of running on a single Nvidia GPU, and gpt-oss-20b, a lighter model optimized for laptop use. The move comes amid growing competition in the global AI market and a push for more open technology in the U.S. and abroad.\n--------------------\nChatGPT’s rapid growth is accelerating. OpenAI said the chatbot was on track to hit 700 million weekly active users in the first week of August, up from 500 million at the end of March. Nick Turley, OpenAI’s VP and head of the ChatGPT app, highlighted the app’s growth on X, noting it has quadrupled in size over the past year.\n--------------------\nOpenAI unveiled Study Mode, a new ChatGPT feature designed to promote critical thinking by prompting students to engage with material rather than simply receive answers. The tool is now rolling out to Free, Plus, Pro, and Team users, with availability for Edu subscribers expected in the coming weeks.\n--------------------\nChatGPT users should be cautious when seeking emotional support from AI, as the AI industry lacks safeguards for sensitive conversations, OpenAI CEO Sam Altman said on a recent episode of This Past Weekend w/ Theo Von. Unlike human therapists, AI tools aren’t bound by doctor-patient confidentiality, he noted.\n--------------------\nChatGPT now receives 2.5 billion prompts daily from users worldwide, including roughly 330 million from the U.S. That’s more than double the volume reported by CEO Sam Altman just eight months ago, highlighting the chatbot’s explosive growth.\n--------------------\nOpenAI has introduced ChatGPT Agent, which completes a wide variety of computer-based tasks on behalf of users and combines several capabilities like Operator and Deep Research, according to the company. OpenAI says the agent can automatically navigate a user’s calendar, draft editable presentations and slideshows, run code, shop online, and handle complex workflows from end to end, all within a secure virtual environment.\n--------------------\nResearchers at Stanford University have observed that therapy chatbots powered by large language models can sometimes stigmatize people with mental health conditions or respond in ways that are inappropriate or could be harmful. While chatbots are “being used as companions, confidants, and therapists,” the study found “significant risks.”\n--------------------\nCEO Sam Altman said that the company is delaying the release of its open model, which had already been postponed by a month earlier this summer. The ChatGPT maker, which initially planned to release the model around mid-July, has indefinitely postponed its launch to conduct additional safety testing.\n--------------------\nOpenAI plans to release an AI-powered web browser to challenge Alphabet’s Google Chrome. It will keep some user interactions within ChatGPT, rather than directing people to external websites.\n--------------------\nSome ChatGPT users have noticed a new feature called “Study Together” appearing in their list of available tools. This is the chatbot’s approach to becoming a more effective educational tool, rather than simply providing answers to prompts. Some people also wonder whether there will be a feature that allows multiple users to join the chat, similar to a study group.\n--------------------\nReferrals from ChatGPT to news publishers are increasing. But this rise is insufficient to offset the decline in clicks as more users now obtain their news directly from AI or AI-powered search results, according to a report by digital market intelligence company Similarweb. Since Google launched its AI Overviews in May 2024, the percentage of news searches that don’t lead to clicks on news websites has increased from 56% to nearly 69% by May 2025.\n--------------------\nOpenAI has started using Google’s AI chips to power ChatGPT and other products, as reported by Reuters. The ChatGPT maker is one of the biggest buyers of Nvidia’s GPUs, using the AI chips to train models, and this is the first time that OpenAI is using non-Nvidia chips in an important way.\n--------------------\nResearchers from MIT’s Media Lab monitored the brain activity of writers in 32 regions. They found that ChatGPT users showed minimal brain engagement and consistently fell short in neural, linguistic, and behavioral aspects. To conduct the test, the lab split 54 participants from the Boston area into three groups, each consisting of individuals ages 18 to 39. The participants were asked to write multiple SAT essays using tools such as OpenAI’s ChatGPT, the Google search engine, or without any tools.\n--------------------\nThe ChatGPT app for iOS was downloaded 29.6 million times in the last 28 days, while TikTok, Facebook, Instagram, and X were downloaded a total of 32.9 million times during the same period, representing a difference of about 10.6%, according to ZDNET report citing Similarweb’s X post.\n--------------------\nSam Altman said that the average ChatGPT query uses about one-fifteenth of a teaspoon of water, equivalent to 0.000083 gallons of water, or the energy required to power a lightbulb for a few minutes, per Business Insider. In addition to that, the chatbot requires 0.34 watt-hours of electricity to operate.\n--------------------\nOpenAI has unveiled o3-pro, an enhanced version of its o3, a reasoning model that the chatGPT maker launched earlier this year. O3-pro is available for ChatGPT and Team users and in the API, while Enterprise and Edu users will get access in the third week of June.\n--------------------\nOpenAI upgraded ChatGPT’s conversational voice mood for all paid users across different markets and platforms. The startup has launched an update to Advanced Voice that enables users to converse with ChatGPT out loud in a more natural and fluid sound. The feature also helps users translate languages more easily, the comapny said.\n--------------------\nOpenAI’s ChatGPT now offers new funtions for business users, including integrations with various cloud services, meeting recordings, and MCP connection support for connecting to tools for in-depth research. The feature enables ChatGPT to retrieve information across users’ own services to answer their questions. For instance, an analyst could use the company’s slide deck and documents to develop an investment thesis.\n--------------------\nOpenAI plans to purchase Jony Ive’s devices startup io for $6.4 billion. Sarah Friar, CFO of OpenAI, thinks that the hardware will significantly enhance ChatGPT and broaden OpenAI’s reach to a larger audience in the future.\n--------------------\nOpenAI has introduced its AI coding agent, Codex, powered by codex-1, a version of its o3 AI reasoning model designed for software engineering tasks. OpenAI says codex-1 generates more precise and “cleaner” code than o3. The coding agent may take anywhere from one to 30 minutes to complete tasks such as writing simple features, fixing bugs, answering questions about your codebase, and running tests.\n--------------------\nSam Altman, the CEO of OpenAI, said during a recent AI event hosted by VC firm Sequoia that he wants ChatGPT to record and remember every detail of a person’s life when one attendee asked about how ChatGPT can become more personalized.\n--------------------\nOpenAI said in a post on X that it has launched its GPT-4.1 and GPT4.1 mini AI models in ChagGPT.\n--------------------\nOpenAI has launched a new feature for ChatGPT deep research to analyze code repositories on GitHub. The ChatGPT deep research feature is in beta and lets developers connect with GitHub to ask questions about codebases and engineering documents. The connector will soon be available for ChatGPT Plus, Pro, and Team users, with support for Enterprise and Education coming shortly, per an OpenAI spokesperson.\n--------------------\nAfter introducing a data residency program in Europe in February, OpenAI has now launched a similar program in Asian countries including India, Japan, Singapore, and South Korea. The new program will be accessible to users of ChatGPT Enterprise, ChatGPT Edu, and API. It will help organizations in Asia meet their local data sovereignty requirements when using OpenAI’s products.\n--------------------\nOpenAI is unveiling a program called OpenAI for Countries, which aims to develop the necessary local infrastructure to serve international AI clients better. The AI startup will work with governments to assist with increasing data center capacity and customizing OpenAI’s products to meet specific language and local needs. OpenAI for Countries is part of efforts to support the company’s expansion of its AI data center Project Stargate to new locations outside the U.S., per Bloomberg.\n--------------------\nOpenAI has announced its plan to make changes to its procedures for updating the AI models that power ChatGPT, following an update that caused the platform to become overly sycophantic for many users.\n--------------------\nOpenAI has released a post on the recent sycophancy issues with the default AI model powering ChatGPT, GPT-4o, leading the company to revert an update to the model released last week. CEO Sam Altman acknowledged the issue on Sunday and confirmed two days later that the GPT-4o update was being rolled back. OpenAI is working on “additional fixes” to the model’s personality. Over the weekend, users on social media criticized the new model for making ChatGPT too validating and agreeable. It became a popular meme fast.\n--------------------\nAn issue within OpenAI’s ChatGPT enabled the chatbot to create graphic erotic content for accounts registered by users under the age of 18, as demonstrated by TechCrunch’s testing, a fact later confirmed by OpenAI. “Protecting younger users is a top priority, and our Model Spec, which guides model behavior, clearly restricts sensitive content like erotica to narrow contexts such as scientific, historical, or news reporting,” a spokesperson told TechCrunch via email. “In this case, a bug allowed responses outside those guidelines, and we are actively deploying a fix to limit these generations.”\n--------------------\nOpenAI has added a few features to its ChatGPT search, its web search tool in ChatGPT, to give users an improved online shopping experience. The company says people can ask super-specific questions using natural language and receive customized results. The chatbot provides recommendations, images, and reviews of products in various categories such as fashion, beauty, home goods, and electronics.\n--------------------\nOpenAI leaders have been talking about allowing the open model to link up with OpenAI’s cloud-hosted models to improve its ability to respond to intricate questions, two sources familiar with the situation told TechCrunch.\n--------------------\nOpenAI is preparing to launch an AI system that will be openly accessible, allowing users to download it for free without any API restrictions. Aidan Clark, OpenAI’s VP of research, is spearheading the development of the open model, which is in the very early stages, sources familiar with the situation told TechCrunch.\n--------------------\nOpenAI released a new AI model called GPT-4.1 in mid-April. However, multiple independent tests indicate that the model is less reliable than previous OpenAI releases. The company skipped that step — sending safety cards for GPT-4.1 — claiming in a statement to TechCrunch that “GPT-4.1 is not a frontier model, so there won’t be a separate system card released for it.”\n--------------------\nQuestions have been raised regarding OpenAI’s transparency and procedures for testing models after a difference in benchmark outcomes was detected by first- and third-party benchmark results for the o3 AI model. OpenAI introduced o3 in December, stating that the model could solve approximately 25% of questions on FrontierMath, a difficult math problem set. Epoch AI, the research institute behind FrontierMath, discovered that o3 achieved a score of approximately 10%, which was significantly lower than OpenAI’s top-reported score.\n--------------------\nOpenAI has launched a new API feature called Flex processing that allows users to use AI models at a lower cost but with slower response times and occasional resource unavailability. Flex processing is available in beta on the o3 and o4-mini reasoning models for non-production tasks like model evaluations, data enrichment, and asynchronous workloads.\n--------------------\nOpenAI has rolled out a new system to monitor its AI reasoning models, o3 and o4 mini, for biological and chemical threats. The system is designed to prevent models from giving advice that could potentially lead to harmful attacks, as stated in OpenAI’s safety report.\n--------------------\nOpenAI has released two new reasoning models, o3 and o4 mini, just two days after launching GPT-4.1. The company claims o3 is the most advanced reasoning model it has developed, while o4-mini is said to provide a balance of price, speed, and performance. The new models stand out from previous reasoning models because they can use ChatGPT features like web browsing, coding, and image processing and generation. But they hallucinate more than several of OpenAI’s previous models.\n--------------------\nOpen AI introduced a new section called “library” to make it easier for users to create images on mobile and web platforms, per the company’s X post.\n--------------------\nOpenAI said on Tuesday that it might revise its safety standards if “another frontier AI developer releases a high-risk system without comparable safeguards.” The move shows how commercial AI developers face more pressure to rapidly implement models due to the increased competition.\n--------------------\nOpenAI is currently in the early stages of developing its own social media platform to compete with Elon Musk’s X and Mark Zuckerberg’s Instagram and Threads, according to The Verge. It is unclear whether OpenAI intends to launch the social network as a standalone application or incorporate it into ChatGPT.\n--------------------\nOpenAI will discontinue its largest AI model, GPT-4.5, from its API even though it was just launched in late February. GPT-4.5 will be available in a research preview for paying customers. Developers can use GPT-4.5 through OpenAI’s API until July 14; then, they will need to switch to GPT-4.1, which was released on April 14.\n--------------------\nOpenAI has launched three members of the GPT-4.1 model — GPT-4.1, GPT-4.1 mini, and GPT-4.1 nano — with a specific focus on coding capabilities. It’s accessible via the OpenAI API but not ChatGPT. In the competition to develop advanced programming models, GPT-4.1 will rival AI models such as Google’s Gemini 2.5 Pro, Anthropic’s Claude 3.7 Sonnet, and DeepSeek’s upgraded V3.\n--------------------\nOpenAI plans to sunset GPT-4, an AI model introduced more than two years ago, and replace it with GPT-4o, the current default model, per changelog. It will take effect on April 30. GPT-4 will remain available via OpenAI’s API.\n--------------------\nOpenAI may launch several new AI models, including GPT-4.1, soon, The Verge reported, citing anonymous sources. GPT-4.1 would be an update of OpenAI’s GPT-4o, which was released last year. On the list of upcoming models are GPT-4.1 and smaller versions like GPT-4.1 mini and nano, per the report.\n--------------------\nOpenAI started updating ChatGPT to enable the chatbot to remember previous conversations with a user and customize its responses based on that context. This feature is rolling out to ChatGPT Pro and Plus users first, excluding those in the U.K., EU, Iceland, Liechtenstein, Norway, and Switzerland.\n--------------------\nIt looks like OpenAI is working on a watermarking feature for images generated using GPT-4o. AI researcher Tibor Blaho spotted a new “ImageGen” watermark feature in the new beta of ChatGPT’s Android app. Blaho also found mentions of other tools: “Structured Thoughts,” “Reasoning Recap,” “CoT Search Tool,” and “l1239dk1.”\n--------------------\nOpenAI is offering its $20-per-month ChatGPT Plus subscription tier for free to all college students in the U.S. and Canada through the end of May. The offer will let millions of students use OpenAI’s premium service, which offers access to the company’s GPT-4o model, image generation, voice interaction, and research tools that are not available in the free version.\n--------------------\nMore than 130 million users have created over 700 million images since ChatGPT got the upgraded image generator on March 25, according to COO of OpenAI Brad Lightcap. The image generator was made available  to all ChatGPT users on March 31, and went viral for being able to create Ghibli-style photos.\n--------------------\nThe Arc Prize Foundation, which develops the AI benchmark tool ARC-AGI, has updated the estimated computing costs for OpenAI’s o3 “reasoning” model managed by ARC-AGI. The organization originally estimated that the best-performing configuration of o3 it tested, o3 high, would cost approximately $3,000 to address a single problem. The Foundation now thinks the cost could be much higher, possibly around $30,000 per task.\n--------------------\nIn a series of posts on X, OpenAI CEO Sam Altman said the company’s new image-generation tool’s popularity may cause product releases to be delayed. “We are getting things under control, but you should expect new releases from OpenAI to be delayed, stuff to break, and for service to sometimes be slow as we deal with capacity challenges,” he wrote.\n--------------------\nOpeanAI intends to release its “first” open language model since GPT-2 “in the coming months.” The company plans to host developer events to gather feedback and eventually showcase prototypes of the model. The first developer event is to be held in San Francisco, with sessions to follow in Europe and Asia.\n--------------------\nOpenAI made a notable change to its content moderation policies after the success of its new image generator in ChatGPT, which went viral for being able to create Studio Ghibli-style images. The company has updated its policies to allow ChatGPT to generate images of public figures, hateful symbols, and racial features when requested. OpenAI had previously declined such prompts due to the potential controversy or harm they may cause. However, the company has now “evolved” its approach, as stated in a blog post published by Joanne Jang, the lead for OpenAI’s model behavior.\n--------------------\nOpenAI wants to incorporate Anthropic’s Model Context Protocol (MCP) into all of its products, including the ChatGPT desktop app. MCP, an open-source standard, helps AI models generate more accurate and suitable responses to specific queries, and lets developers create bidirectional links between data sources and AI applications like chatbots. The protocol is currently available in the Agents SDK, and support for the ChatGPT desktop app and Responses API will be coming soon, OpenAI CEO Sam Altman said.\n--------------------\nThe latest update of the image generator on OpenAI’s ChatGPT has triggered a flood of AI-generated memes in the style of Studio Ghibli, the Japanese animation studio behind blockbuster films like “My Neighbor Totoro” and “Spirited Away.” The burgeoning mass of Ghibli-esque images have sparked concerns about whether OpenAI has violated copyright laws, especially since the company is already facing legal action for using source material without authorization.\n--------------------\nOpenAI expects its revenue to triple to $12.7 billion in 2025, fueled by the performance of its paid AI software, Bloomberg reported, citing an anonymous source. While the startup doesn’t expect to reach positive cash flow until 2029, it expects revenue to increase significantly in 2026 to surpass $29.4 billion, the report said.\n--------------------\nOpenAI on Tuesday rolled out a major upgrade to ChatGPT’s image-generation capabilities: ChatGPT can now use the GPT-4o model to generate and edit images and photos directly. The feature went live earlier this week in ChatGPT and Sora, OpenAI’s AI video-generation tool, for subscribers of the company’s Pro plan, priced at $200 a month, and will be available soon to ChatGPT Plus subscribers and developers using the company’s API service. The company’s CEO Sam Altman said on Wednesday, however, that the release of the image generation feature to free users would be delayed due to higher demand than the company expected.\n--------------------\nBrad Lightcap, OpenAI’s chief operating officer, will lead the company’s global expansion and manage corporate partnerships as CEO Sam Altman shifts his focus to research and products, according to a blog post from OpenAI. Lightcap, who previously worked with Altman at Y Combinator, joined the Microsoft-backed startup in 2018. OpenAI also said Mark Chen would step into the expanded role of chief research officer, and Julia Villagra will take on the role of chief people officer.\n--------------------\nOpenAI has updated its AI voice assistant with improved chatting capabilities, according to a video posted on Monday (March 24) to the company’s official media channels. The update enables real-time conversations, and the AI assistant is said to be more personable and interrupts users less often. Users on ChatGPT’s free tier can now access the new version of Advanced Voice Mode, while paying users will receive answers that are “more direct, engaging, concise, specific, and creative,” a spokesperson from OpenAI told TechCrunch.\n--------------------\nOpenAI and Meta have separately engaged in discussions with Indian conglomerate Reliance Industries regarding potential collaborations to enhance their AI services in the country, per a report by The Information. One key topic being discussed is Reliance Jio distributing OpenAI’s ChatGPT. Reliance has proposed selling OpenAI’s models to businesses in India through an application programming interface (API) so they can incorporate AI into their operations. Meta also plans to bolster its presence in India by constructing a large 3GW data center in Jamnagar, Gujarat. OpenAI, Meta, and Reliance have not yet officially announced these plans.\n--------------------\nNoyb, a privacy rights advocacy group, is supporting an individual in Norway who was shocked to discover that ChatGPT was providing false information about him, stating that he had been found guilty of killing two of his children and trying to harm the third. “The GDPR is clear. Personal data has to be accurate,” said Joakim Söderberg, data protection lawyer at Noyb, in a statement. “If it’s not, users have the right to have it changed to reflect the truth. Showing ChatGPT users a tiny disclaimer that the chatbot can make mistakes clearly isn’t enough. You can’t just spread false information and in the end add a small disclaimer saying that everything you said may just not be true.”\n--------------------\nOpenAI has added new transcription and voice-generating AI models to its APIs: a text-to-speech model, “gpt-4o-mini-tts,” that delivers more nuanced and realistic sounding speech, as well as two speech-to-text models called “gpt-4o-transcribe” and “gpt-4o-mini-transcribe”. The company claims they are improved versions of what was already there and that they hallucinate less.\n--------------------\nOpenAI has introduced o1-pro in its developer API. OpenAI says its o1-pro uses more computing than its o1 “reasoning” AI model to deliver “consistently better responses.” It’s only accessible to select developers who have spent at least $5 on OpenAI API services. OpenAI charges $150 for every million tokens (about 750,000 words) input into the model and $600 for every million tokens the model produces. It costs twice as much as OpenAI’s GPT-4.5 for input and 10 times the price of regular o1.\n--------------------\nNoam Brown, who heads AI reasoning research at OpenAI, thinks that certain types of AI models for “reasoning” could have been developed 20 years ago if researchers had understood the correct approach and algorithms.\n--------------------\nOpenAI CEO Sam Altman said, in a post on X, that the company has trained a “new model” that’s “really good” at creative writing. He posted a lengthy sample from the model given the prompt “Please write a metafictional literary short story about AI and grief.” OpenAI has not extensively explored the use of AI for writing fiction. The company has mostly concentrated on challenges in rigid, predictable areas such as math and programming. And it turns out that it might not be that great at creative writing at all.\n--------------------\nOpenAI rolled out new tools designed to help developers and businesses build AI agents — automated systems that can independently accomplish tasks — using the company’s own AI models and frameworks. The tools are part of OpenAI’s new Responses API, which enables enterprises to develop customized AI agents that can perform web searches, scan through company files, and navigate websites, similar to OpenAI’s Operator product. The Responses API effectively replaces OpenAI’s Assistants API, which the company plans to discontinue in the first half of 2026.\n--------------------\nOpenAI intends to release several “agent” products tailored for different applications, including sorting and ranking sales leads and software engineering, according to a report from The Information. One, a “high-income knowledge worker” agent, will reportedly be priced at $2,000 a month. Another, a software developer agent, is said to cost $10,000 a month. The most expensive rumored agents, which are said to be aimed at supporting “PhD-level research,” are expected to cost $20,000 per month. The jaw-dropping figure is indicative of how much cash OpenAI needs right now: The company lost roughly $5 billion last year after paying for costs related to running its services and other expenses. It’s unclear when these agentic tools might launch or which customers will be eligible to buy them.\n--------------------\nThe latest version of the macOS ChatGPT app allows users to edit code directly in supported developer tools, including Xcode, VS Code, and JetBrains. ChatGPT Plus, Pro, and Team subscribers can use the feature now, and the company plans to roll it out to more users like Enterprise, Edu, and free users.\n--------------------\nAccording to a new report from VC firm Andreessen Horowitz (a16z), OpenAI’s AI chatbot, ChatGPT, experienced solid growth in the second half of 2024. It took ChatGPT nine months to increase its weekly active users from 100 million in November 2023 to 200 million in August 2024, but it only took less than six months to double that number once more, according to the report. ChatGPT’s weekly active users increased to 300 million by December 2024 and 400 million by February 2025. ChatGPT has experienced significant growth recently due to the launch of new models and features, such as GPT-4o, with multimodal capabilities. ChatGPT usage spiked from April to May 2024, shortly after that model’s launch.\n--------------------\nOpenAI has effectively canceled the release of o3 in favor of what CEO Sam Altman is calling a “simplified” product offering. In a post on X, Altman said that, in the coming months, OpenAI will release a model called GPT-5 that “integrates a lot of [OpenAI’s] technology,” including o3, in ChatGPT and its API. As a result of that roadmap decision, OpenAI no longer plans to release o3 as a standalone model.\n--------------------\nA commonly cited stat is that ChatGPT requires around 3 watt-hours of power to answer a single question. Using OpenAI’s latest default model for ChatGPT, GPT-4o, as a reference, nonprofit AI research institute Epoch AI found the average ChatGPT query consumes around 0.3 watt-hours. However, the analysis doesn’t consider the additional energy costs incurred by ChatGPT with features like image generation or input processing.\n--------------------\nIn response to pressure from rivals like DeepSeek, OpenAI is changing the way its o3-mini model communicates its step-by-step “thought” process. ChatGPT users will see an updated “chain of thought” that shows more of the model’s “reasoning” steps and how it arrived at answers to questions.\n--------------------\nOpenAI is now allowing anyone to use ChatGPT web search without having to log in. While OpenAI had previously allowed users to ask ChatGPT questions without signing in, responses were restricted to the chatbot’s last training update. This only applies through ChatGPT.com, however. To use ChatGPT in any form through the native mobile app, you will still need to be logged in.\n--------------------\nOpenAI announced a new AI “agent” called deep research that’s designed to help people conduct in-depth, complex research using ChatGPT. OpenAI says the “agent” is intended for instances where you don’t just want a quick answer or summary, but instead need to assiduously consider information from multiple websites and other sources.\n--------------------\nOpenAI used the subreddit r/ChangeMyView to measure the persuasive abilities of its AI reasoning models. OpenAI says it collects user posts from the subreddit and asks its AI models to write replies, in a closed environment, that would change the Reddit user’s mind on a subject. The company then shows the responses to testers, who assess how persuasive the argument is, and finally OpenAI compares the AI models’ responses to human replies for that same post.\n--------------------\nOpenAI launched a new AI “reasoning” model, o3-mini, the newest in the company’s o family of models. OpenAI first previewed the model in December alongside a more capable system called o3. OpenAI is pitching its new model as both “powerful” and “affordable.”\n--------------------\nA new report from app analytics firm Appfigures found that over half of ChatGPT’s mobile users are under age 25, with users between ages 50 and 64 making up the second largest age demographic. The gender gap among ChatGPT users is even more significant. Appfigures estimates that across age groups, men make up 84.5% of all users.\n--------------------\nOpenAI launched ChatGPT Gov designed to provide U.S. government agencies an additional way to access the tech. ChatGPT Gov includes many of the capabilities found in OpenAI’s corporate-focused tier, ChatGPT Enterprise. OpenAI says that ChatGPT Gov enables agencies to more easily manage their own security, privacy, and compliance, and could expedite internal authorization of OpenAI’s tools for the handling of non-public sensitive data.\n--------------------\nYounger Gen Zers are embracing ChatGPT, for schoolwork, according to a new survey by the Pew Research Center. In a follow-up to its 2023 poll on ChatGPT usage among young people, Pew asked ~1,400 U.S.-based teens ages 13 to 17 whether they’ve used ChatGPT for homework or other school-related assignments. Twenty-six percent said that they had, double the number two years ago. Just over half of teens responding to the poll said they think it’s acceptable to use ChatGPT for researching new subjects. But considering the ways ChatGPT can fall short, the results are possibly cause for alarm.\n--------------------\nOpenAI says that it might store chats and associated screenshots from customers who use Operator, the company’s AI “agent” tool, for up to 90 days — even after a user manually deletes them. While OpenAI has a similar deleted data retention policy for ChatGPT, the retention period for ChatGPT is only 30 days, which is 60 days shorter than Operator’s.\n--------------------\nOpenAI is launching a research preview of Operator, a general-purpose AI agent that can take control of a web browser and independently perform certain actions. Operator promises to automate tasks such as booking travel accommodations, making restaurant reservations, and shopping online.\n--------------------\nOperator, OpenAI’s agent tool, could be released sooner rather than later. Changes to ChatGPT’s code base suggest that Operator will be available as an early research preview to users on the $200 Pro subscription plan. The changes aren’t yet publicly visible, but a user on X who goes by Choi spotted these updates in ChatGPT’s client-side code. TechCrunch separately identified the same references to Operator on OpenAI’s website.\n--------------------\nOpenAI has begun testing a feature that lets new ChatGPT users sign up with only a phone number — no email required. The feature is currently in beta in the U.S. and India. However, users who create an account using their number can’t upgrade to one of OpenAI’s paid plans without verifying their account via an email. Multi-factor authentication also isn’t supported without a valid email.\n--------------------\nChatGPT’s new beta feature, called tasks, allows users to set simple reminders. For example, you can ask ChatGPT to remind you when your passport expires in six months, and the AI assistant will follow up with a push notification on whatever platform you have tasks enabled. The feature will start rolling out to ChatGPT Plus, Team, and Pro users around the globe this week.\n--------------------\nOpenAI is introducing a new way for users to customize their interactions with ChatGPT. Some users found they can specify a preferred name or nickname and “traits” they’d like the chatbot to have. OpenAI suggests traits like “Chatty,” “Encouraging,” and “Gen Z.” However, some users reported that the new options have disappeared, so it’s possible they went live prematurely.\n--------------------\nChatGPT is a general-purpose chatbot that uses artificial intelligence to generate text after a user enters a prompt, developed by tech startup OpenAI. The chatbot uses GPT-4, a large language model that uses deep learning to produce human-like text.\n--------------------\nNovember 30, 2022 is when ChatGPT was released for public use.\n--------------------\nBoth the free version of ChatGPT and the paid ChatGPT Plus are regularly updated with new GPT models. The most recent model is GPT-4o.\n--------------------\nThere is a free version of ChatGPT that only requires a sign-in in addition to the paid version, ChatGPT Plus.\n--------------------\nAnyone can use ChatGPT! More and more tech companies and search engines are utilizing the chatbot to automate text or quickly answer user questions/concerns.\n--------------------\nMultiple enterprises utilize ChatGPT, although others may limit the use of the AI-powered tool.\n--------------------\nMost recently, Microsoft announced at its 2023 Build conference that it is integrating its ChatGPT-based Bing experience into Windows 11. A Brooklyn-based 3D display startup Looking Glass utilizes ChatGPT to produce holograms you can communicate with by using ChatGPT.  And nonprofit organization Solana officially integrated the chatbot into its network with a ChatGPT plug-in geared toward end users to help onboard into the web3 space.\n--------------------\nGPT stands for Generative Pre-Trained Transformer.\n--------------------\nA chatbot can be any software/system that holds dialogue with you/a person but doesn’t necessarily have to be AI-powered. For example, there are chatbots that are rules-based in the sense that they’ll give canned responses to questions.\n--------------------\nChatGPT is AI-powered and utilizes LLM technology to generate text after a prompt.\n--------------------\nYes.\n--------------------\nDue to the nature of how these models work, they don’t know or care whether something is true, only that it looks true. That’s a problem when you’re using it to do your homework, sure, but when it accuses you of a crime you didn’t commit, that may well at this point be libel.\n--------------------\nWe will see how handling troubling statements produced by ChatGPT will play out over the next few months as tech and legal experts attempt to tackle the fastest moving target in the industry.\n--------------------\nYes, there is a free ChatGPT mobile app for iOS and Android users.\n--------------------\nIt’s not documented anywhere that ChatGPT has a character limit. However, users have noted that there are some character limitations after around 500 words.\n--------------------\nYes, it was released March 1, 2023.\n--------------------\nEveryday examples include programming, scripts, email replies, listicles, blog ideas, summarization, etc.\n--------------------\nAdvanced use examples include debugging code, programming languages, scientific concepts, complex problem solving, etc.\n--------------------\nIt depends on the nature of the program. While ChatGPT can write workable Python code, it can’t necessarily program an entire app’s worth of code. That’s because ChatGPT lacks context awareness — in other words, the generated code isn’t always appropriate for the specific context in which it’s being used.\n--------------------\nYes. OpenAI allows users to save chats in the ChatGPT interface, stored in the sidebar of the screen. There are no built-in sharing features yet.\n--------------------\nYes. There are multiple AI-powered chatbot competitors such as Together, Google’s Gemini and Anthropic’s Claude, and developers are creating open source alternatives.\n--------------------\nOpenAI has said that individuals in “certain jurisdictions” (such as the EU) can object to the processing of their personal information by its AI models by filling out this form. This includes the ability to make requests for deletion of AI-generated references about you. Although OpenAI notes it may not grant every request since it must balance privacy requests against freedom of expression “in accordance with applicable laws”.\n--------------------\nThe web form for making a deletion of data about you request is entitled “OpenAI Personal Data Removal Request”.\n--------------------\nIn its privacy policy, the ChatGPT maker makes a passing acknowledgement of the objection requirements attached to relying on “legitimate interest” (LI), pointing users towards more information about requesting an opt out — when it writes: “See here for instructions on how you can opt out of our use of your information to train our models.”\n--------------------\nRecently, Discord announced that it had integrated OpenAI’s technology into its bot named Clyde where two users tricked Clyde into providing them with instructions for making the illegal drug methamphetamine (meth) and the incendiary mixture napalm.\n--------------------\nAn Australian mayor has publicly announced he may sue OpenAI for defamation due to ChatGPT’s false claims that he had served time in prison for bribery. This would be the first defamation lawsuit against the text-generating service.\n--------------------\nCNET found itself in the midst of controversy after Futurism reported the publication was publishing articles under a mysterious byline completely generated by AI. The private equity company that owns CNET, Red Ventures, was accused of using ChatGPT for SEO farming, even if the information was incorrect.\n--------------------\nSeveral major school systems and colleges, including New York City Public Schools, have banned ChatGPT from their networks and devices. They claim that the AI impedes the learning process by promoting plagiarism and misinformation, a claim that not every educator agrees with.\n--------------------\nThere have also been cases of ChatGPT accusing individuals of false crimes.\n--------------------\nSeveral marketplaces host and provide ChatGPT prompts, either for free or for a nominal fee. One is PromptBase. Another is ChatX. More launch every day.\n--------------------\nPoorly. Several tools claim to detect ChatGPT-generated text, but in our tests, they’re inconsistent at best.\n--------------------\nNo. But OpenAI recently disclosed a bug, since fixed, that exposed the titles of some users’ conversations to other people on the service.\n--------------------\nNone specifically targeting ChatGPT. But OpenAI is involved in at least one lawsuit that has implications for AI systems trained on publicly available data, which would touch on ChatGPT.\n--------------------\nYes. Text-generating AI models like ChatGPT have a tendency to regurgitate content from their training data.\n--------------------\nThis story is continually updated with new information.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Google and PayPal team up on agentic commerce",
    "url": "https://techcrunch.com/2025/09/18/google-and-paypal-team-up-on-agentic-commerce/",
    "content": "Posted:\n--------------------\nPayPal announced on Wednesday a new multi-year partnership with Google that will see the payments giant using Google’s AI technology to create new AI-powered shopping experiences. PayPal’s solutions, meanwhile, will be integrated across Google’s products, and PayPal will work with Google Cloud on hosting and improving its technology infrastructure.\n--------------------\nThe companies did not detail what specific types of agentic shopping experiences they would work together to create but said Google would contribute its AI technology and expertise, and PayPal would leverage its global payment infrastructure, personalization, and identity solutions.\n--------------------\nIn addition, both companies will join others to advocate for the adoption of Google’s new Agent Payments Protocol, announced on Tuesday. This open protocol is meant to enable purchases that are initiated by AI agents and has already been backed by over 60 merchants and financial institutions.\n--------------------\nAs part of the deal, PayPal will be listed as a key payment provider for card payments in areas like Google Cloud, Google Ads, and Google Play. Other products being integrated by Google include PayPal’s branded checkout, Hyperwallet payouts service, and PayPal Payouts service.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Nvidia buys $5B stake in Intel, planning AI chip collaboration",
    "url": "https://techcrunch.com/2025/09/18/nvidia-buys-5-billion-stake-in-intel-planning-ai-chip-collaboration/",
    "content": "Nvidia has agreed to buy a $5 billion stake in Intel as part of a broader deal to together develop “multiple generations” of data center and PC products.\n--------------------\nNvidia will acquire the Intel stock for $23.28 per share, a slight discount on the company’s previous trading price. According to Reuters, the deal would make Nvidia one of Intel’s largest shareholders, owning about 4% of the company. Intel shares were up as much as 30% in early trading on Thursday morning.\n--------------------\nThe companies will integrate their two architectures using Nvidia’s NVLink interface, which enables data and control code transfers between CPUs and GPUs. NVLink enables faster transfers between chips compared with other standards like PCI Express; that’s crucial for AI applications, which require many GPUs to run together and process immense workloads.\n--------------------\nFor data centers, Intel will manufacture a new line of x86 CPUs specifically customized for Nvidia’s AI infrastructure platforms, to be offered to enterprise and hyperscale customers.\n--------------------\nFor the consumer PC segment, Intel will build x86 system-on-chips that will incorporate chiplets of Nvidia’s RTX GPUs, which will no doubt give Intel an edge over rival AMD’s CPUs. The companies are calling these chips “x86 RTX SoCs” at the moment and claim these chips will power a “wide range of PCs.”\n--------------------\nThe deal comes after a rough few years for Intel, which has struggled to capitalize on the AI chip race unlike its new partner. Intel brought on a new CEO, laid off thousands of staff as it sought to shore up margins, and spiked manufacturing projects to prioritize more financial discipline.\n--------------------\nThe deal comes on the heels of another record quarter for Nvidia, which has grown into both the world’s most lucrative semiconductor company and, by market cap, one of the largest companies in the world. Over the same period, Intel has struggled to keep pace with the fluctuations of market demand, particularly the intense semiconductor demands of AI. As a result, the collaboration could allow Intel to win back market share from rivals like AMD.\n--------------------\n“Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement NVIDIA’s AI and accelerated computing leadership to enable new breakthroughs for the industry,” said Intel CEO Lip-Bu Tan in a statement.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Building the future of Open AI with Thomas Wolf at TechCrunch Disrupt 2025",
    "url": "https://techcrunch.com/2025/09/18/building-the-future-of-open-ai-with-thomas-wolf-at-techcrunch-disrupt-2025/",
    "content": "AI is moving fast — but who decides how it’s built, shared, and scaled? At TechCrunch Disrupt 2025, happening October 27-29 at San Francisco’s Moscone West, we’re diving in with Thomas Wolf, co-founder and chief science officer of Hugging Face, for a session on what it takes to make cutting-edge research and models that are truly open and accessible. Catch him on the AI Stage, along with thousands of other like-minded tech leaders and innovators.\n--------------------\nThe future of AI won’t just be defined by closed labs and Big Tech budgets — it will be written in open source repos, global research collaborations, and moonshot experiments that anyone can build on. Wolf brings a rare perspective on how to bridge research, community, and real-world applications in ways that shape both the technology and the ecosystem around it.\n--------------------\nWhether you’re a founder, developer, or investor, this session offers a clear view of where AI is headed — and how openness can drive the next wave of breakthroughs.\n--------------------\nWolf has been at the center of some of the most important advances in AI. At Hugging Face, he helped launch the Transformers and Datasets libraries, led the BigScience Workshop on large language models that produced BLOOM, and has championed open science across industry and academia. He’s also co-author of “Natural Language Processing with Transformers” and “The Ultra-Scale Playbook,” resources shaping how the next generation of AI practitioners learn and build.\n--------------------\nJoin 10,000+ startup and VC leaders October 27–29 at Moscone West in San Francisco, with headline-making sessions, hands-on discussions, and intentional networking designed to fuel your next move. Disrupt 2025 is where the future of tech gets built and where we celebrate 20 years of TechCrunch. Lock in your pass now to save up to $668 before savings end on September 26 at 11:59 p.m. PT.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "China tells its tech companies they can’t buy AI chips from Nvidia",
    "url": "https://techcrunch.com/2025/09/17/china-tells-its-tech-companies-they-cant-buy-ai-chips-from-nvidia/",
    "content": "Nvidia just got shut out of the Chinese market — this time by the Chinese government instead of the US.\n--------------------\nChina’s internet regulator, the Cyberspace Administration of China, banned domestic tech companies from buying Nvidia AI chips on Wednesday, as first reported by the Financial Times.\n--------------------\nThe agency also told tech companies including ByteDance and Alibaba to stop testing and ordering Nvidia’s RTX Pro 6000D server, a device designed specifically for the market in China.\n--------------------\nBeijing had previously discouraged companies from buying these chips in late August, instead promoting alternatives from local manufacturers.\n--------------------\nThis ban will deliver quite a blow to China’s tech ecosystem. While companies like Huawei and Alibaba design AI chips locally, Nvidia is by far the global market leader, and its chips are considered to be some of the most advanced on the market.\n--------------------\nWhen asked for comment, Nvidia provided the following statement from CEO Jensen Huang at a press conference on Wednesday: “We can only be in service of a market if a country wants us to be,” Huang said. “I’m disappointed with what I see but they have larger agendas to work out between China and the United States. And I’m patient about it. We’ll continue to be supportive of the Chinese government and Chinese companies as they wish.”\n--------------------\nThe Trump administration hit semiconductor companies, including Nvidia, with licensing requirements to sell their AI chips in China in April.\n--------------------\nOn Nvidia’s first-quarter earnings call, Huang had said Nvidia was going to endure $8 billion of revenue loss in the second quarter alone by not being able to sell its H20 AI chips in China.\n--------------------\nIn June, Nvidia said that it wouldn’t include China in its future profit and forecast as it was essentially locked out of the market.\n--------------------\nIn July, the Trump administration reversed course and gave semiconductor companies the green light to sell their chips in China again. In August, the White House announced it would grant the licenses needed to sell in China, but with a catch: the U.S. government would get 15% of the revenue from the chips sold. But as of Nvidia’s latest earnings, the company had yet to sell any units to Chinese customers under the plan, citing the slow implementation of President Trump’s proposal.\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  },
  {
    "title": "Meta Ray-Ban Display and everything else unveiled at Meta Connect 2025",
    "url": "https://techcrunch.com/2025/09/16/meta-connect-2025-what-to-expect-and-how-to-watch/",
    "content": "At Meta Connect 2025, the company’s biggest event of the year, Mark Zuckerberg unveiled three new smart glasses: the second generation Ray-Ban Meta, the Meta Ray-Ban Display and wristband controller, and the Oakley Meta Vanguard.\n--------------------\nMeta says it has sold two million of the first generation Ray-Ban Meta smart glasses, and earlier this year, Meta unveiled its latest AI-powered smart glasses with Oakley, which were designed for athletes. Silicon Valley is leaning heavily into AI wearables, and Meta seems to be one of the companies leading the charge.\n--------------------\nWith Meta looking to regain its footing in the AI race and sell more hardware, the company had a lot at stake during Mark Zuckerberg’s Meta Connect 2025 keynote. Overall, Meta showcased some pretty impressive technology — the Meta Neural Band, the wristband controller that comes with the Meta Ray-Ban Display, is a particular highlight.\n--------------------\nAnd yet, in a twist that felt reminiscent of HBO’s Silicon Valley, Zuckerberg’s demo of the AI capabilities on the Ray-Ban Metas failed. Whoops!\n--------------------\nWhile sharing a live video feed of the cooking content creator Jack Mancuso at Meta HQ, Zuckerberg asked the chef to demonstrate how his Ray-Ban Meta glasses could help him whip up a Korean-inspired steak sauce.\n--------------------\n“I love the set-up you have here, with soy sauce and other ingredients. How can I help?” asked the chipper Meta AI voice.\n--------------------\nMancuso asked for a recipe for a Korean-inspired steak sauce, and the AI voice began to list the ingredients that he would need — but Mancuso knows he needs to keep the demo succinct, so he interrupts and asks, “What do I do first?”\n--------------------\nAfter a moment of silence that dragged a bit too long, Mancuso repeated, “What do I do first?”\n--------------------\n“You’ve already combined the base ingredients, so now grate a pear to add to the sauce,” the AI said. But he had not yet combined the base ingredients, because he had not started making the recipe, hence the question of what to do first.\n--------------------\nMancuso asked the same question again, and the AI gave the same response. The audience laughed.\n--------------------\n“I think the Wi-Fi might be messed up — back to you Mark!” Mancuso said.\n--------------------\n“You know what? It’s all good. The irony of the whole thing is that you spend years making the technology, and then the Wi-Fi of the day kind of… catches you,” Zuckerberg said. “Anyway, we’ll go check out what he made later.”\n--------------------\nThe whole interaction was a bit awkward, especially since the issue did not seem to be with the Wi-Fi. But even when things are going according to plan, these presentations can feel a bit hokey… like the end of the keynote, when Zuckerberg and Diplo quite literally ran into the sunset together, wearing their Meta Oakley Vanguards. It had to be a busy day for Mark, so maybe he just needed an excuse to build some cardio into his schedule.\n--------------------\nMeta unveiled the second generation of its Ray-Ban Meta glasses, which first debuted in 2023. This spruced-up model features double the battery life of its predecessor, now lasting up to eight hours of mixed use on one charge. The second generation glasses also support ultra HD 3K video recording, which the company says is twice as sharp as the last model.\n--------------------\nMeta’s smart glasses are also getting some new features with the release of the second generation Ray-Ban Metas, like conversation focus, which will be available on the Ray-Ban Meta and Oakley Meta HSTN glasses.\n--------------------\n“If you’re eating at a hot new restaurant, commuting on the train, or catching your favorite DJ’s latest set, conversation focus uses your AI glasses’ open-ear speakers to amplify the voice of the person you’re talking to,” Meta said in its press release.\n--------------------\nConversation focus isn’t out just yet, so we can’t say for sure if it’ll be any help for your next night out.\n--------------------\nThe Live AI feature — which Meta failed to properly demo on stage — is also on its way. But it’s so energy intensive that you can only use it for about an hour or two.\n--------------------\n“As we make battery and energy efficiency optimizations, Meta AI will transition from something you prompt with a wake word to an always-available assistant,” the company said.\n--------------------\nThe second generation Ray-Ban Meta are priced at $379.\n--------------------\nThe Meta Ray-Ban Display are the most impressive glasses that Meta has unveiled to date, featuring a built-in display for apps, alerts, and directions on the right lens. But what sets this pair of smart glasses apart is its accompanying wristband controller, the Meta Neural Band.\n--------------------\nThis wristband lets Meta show off a bit of what it’s been spending so much time (and money) on in its Reality Labs division, which is notorious for losing billions of dollars a quarter.\n--------------------\nVisually, the Meta Neural Band looks like a Fitbit without a screen. It’s powered by surface electromyography (sEMG), which can pick up on minute hand gestures and small movements. This is far more sophisticated than a wrist gesture on an Apple Watch. Users can write out text messages by holding their fingers together as if they were gripping a pen and “writing” out the text. This means that you can see a WhatsApp message come in on your right glasses lens, then answer it by “writing” your response.\n--------------------\nFor now, the glasses support Meta apps, but the company will have to support a wide variety of apps in the future to get the kind of adoption they’re looking for. Like Apple and Google, Meta is betting that smart glasses could cut into the market share of the smart phone in the future — but it will be a big challenge to force such a massive cultural shift.\n--------------------\nThe Meta Ray-Ban Display, which comes with the Meta Neural Band, will cost $799 and launch on September 30.\n--------------------\nFor the bearish among us, it seems hard to imagine wearing smart glasses and sending text messages by handwriting in the air. But the Oakley Meta Vanguard smart glasses, which are designed for athletes, offer the most coherent use case yet for this kind of technology.\n--------------------\nBikers, trail runners, and skiiers can photograph their adventures without pulling out their phones; the glasses’ open-air speakers can play music during your workout, and even link with apps like Strava and Garmin to relay your stats. Like the other new glasses, the Vanguard model is also AI-enabled.\n--------------------\nUnlike other models of Meta smart glasses, the Oakley Meta Vanguards have just one unified front lens with a camera in the middle, rather than two lenses with cameras on either side — it’s a design that makes more technical sense, and it’s a fashion statement that you can pull off in athletic eyewear, but not in eyeglasses (prove me wrong). The new glasses can capture video in up to 3K resolution and feature a 12-megapixel camera with a 122-degree wide-angle lens.\n--------------------\nThe glasses have an IP67 dust and water resistance rating for use during intense workouts. Meta says the wraparound design of the glasses features Oakley PRIZMTM Lens technology, which is designed to block out sun, wind, and dust.\n--------------------\nUnless if you’re an ultra marathon runner, these glasses will easily last throughout your workout. The glasses can stay on for nine hours, or six hours with continuous music playback. But the charging case that the glasses come with can provide an additional 36 hours of charge on the go. Meta claims that the charging case can quickly get the glasses to a 50% charge in 20 minutes.\n--------------------\nThe Oakley Meta Vanguard glasses retail for $499 and go on sale October 21.\n--------------------\nOn the VR front, Meta did not release any new Quest headsets as part of this year’s Connect. Even though the conference, and company, is named after the Metaverse, we learned about just a small number of updates to its VR, such as Hyperscape, which will allow developers and creators to build photorealistic spaces in virtual reality.\n--------------------\nMeta is reportedly developing an ultralight VR headset for launch by the end of 2026, so maybe we’ll see that come to fruition at the next Meta Connect event.\n--------------------\n\n--------------------\nTopics\n--------------------\n© 2025 TechCrunch Media LLC.\n--------------------\n"
  }
]